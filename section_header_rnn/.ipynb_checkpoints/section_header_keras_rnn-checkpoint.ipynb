{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section header classiifcation using keras and tensorflow\n",
    "https://github.com/fchollet/keras/tree/master/examples\n",
    "https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/model.py\n",
    "\n",
    "https://github.com/offbit/char-models/blob/master/doc-rnn2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "from keras.layers import Input, Dense, Embedding, merge, Convolution2D, MaxPooling2D, Dropout, Activation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model,Sequential\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import numpy as np\n",
    "import skflow\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding, LSTM,Lambda,Conv1D,MaxPooling1D,Bidirectional,TimeDistributed\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "from keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#datafile= \"data_for_weka_all.csv\"\n",
    "datafile1= \"../s3/training_data/train_sample/train.part1.csv\"\n",
    "datafile2= \"../s3/training_data/train/train.part2.csv\"\n",
    "datafile3= \"../s3/training_data/test/test.part501.csv\"\n",
    "\n",
    "duplicate_samples=1\n",
    "duplicate_samples_pos=1\n",
    "\n",
    "checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert section header dataset for rnn neural network\n",
    "\n",
    "# data for training and test where we may duplicate positive samples. We may also split the data \n",
    "# into two set. training and test. This ae for moel tesing. \n",
    "sh_dataset = defaultdict(lambda : None)\n",
    "sh_dataset['target_names'] =['no','yes']\n",
    "sh_dataset['target'] =[]\n",
    "sh_dataset['data'] =[]\n",
    "\n",
    "# real  data is for tesing the model on a different sample set\n",
    "real_dataset = defaultdict(lambda : None)\n",
    "real_dataset['target_names'] =['no','yes']\n",
    "real_dataset['target'] =[]\n",
    "real_dataset['data'] =[]\n",
    "\n",
    "\n",
    "\n",
    "with open(datafile1, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(duplicate_samples_pos):\n",
    "                    sh_dataset['target'].append(1)\n",
    "                    sh_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                sh_dataset['target'].append(0)  \n",
    "                sh_dataset['data'].append(row['text'])\n",
    "with open(datafile2, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(duplicate_samples_pos):\n",
    "                    sh_dataset['target'].append(1)\n",
    "                    sh_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                sh_dataset['target'].append(0)  \n",
    "                sh_dataset['data'].append(row['text'])\n",
    "\n",
    "with open(datafile3, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(1): # as in relad test, we don't need to dupliate positive samples\n",
    "                    real_dataset['target'].append(1)\n",
    "                    real_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                real_dataset['target'].append(0)  \n",
    "                real_dataset['data'].append(row['text'])\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train, y_train = sh_dataset['data'], sh_dataset['target']\n",
    "#X_test,  y_test = real_dataset['data'], real_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 100\n",
    "\n",
    "char_processor = skflow.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "X_train = np.array(list(char_processor.fit_transform(X_train)))\n",
    "X_test = np.array(list(char_processor.transform(X_test)))\n",
    "\n",
    "print X_train.shape\n",
    "#print y_train.shape\n",
    "print X_test.shape\n",
    "#print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt = ''\n",
    "docs = []\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "for cont, sentiment in zip(sh_dataset['data'],sh_dataset['target']):\n",
    "    sentences = cont\n",
    "    docs.append(sentences)\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "for doc in docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = set(txt)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 16\n",
    "max_sentences = 5\n",
    "\n",
    "X = np.ones((len(docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "y = np.array(sentiments)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                X[i, j, (maxlen-1-t)] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('acc'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binarize(x, sz=71):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))\n",
    "def binarize_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X[:10000]\n",
    "X_test = X[10000:]\n",
    "\n",
    "y_train = y[:10000]\n",
    "y_test = y[10000:]\n",
    "\n",
    "filter_length = [5, 3, 3]\n",
    "nb_filter = [196, 196, 256]\n",
    "pool_length = 1\n",
    "\n",
    "\n",
    "in_sentence = Input(shape=(maxlen,), dtype='int64')\n",
    "# binarize function creates a onehot encoding of each character index\n",
    "embedded = Lambda(binarize, output_shape=binarize_outshape)(in_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(nb_filter)):\n",
    "    embedded = Conv1D(nb_filter=nb_filter[i],\n",
    "                            filter_length=filter_length[i],\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            init='glorot_normal',\n",
    "                            subsample_length=1)(embedded)\n",
    "\n",
    "    embedded = Dropout(0.1)(embedded)\n",
    "    embedded = MaxPooling1D(pool_length=pool_length)(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bi_lstm_sent = Bidirectional(LSTM(128, return_sequences=False, dropout=0.15, recurrent_dropout=0.15, implementation=0))(embedded)\n",
    "\n",
    "# sent_encode = merge([forward_sent, backward_sent], mode='concat', concat_axis=-1)\n",
    "sent_encode = Dropout(0.3)(bi_lstm_sent)\n",
    "# sentence encoder\n",
    "encoder = Model(inputs=in_sentence, outputs=sent_encode)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document = Input(shape=(max_sentences, maxlen), dtype='int64')\n",
    "encoded = TimeDistributed(encoder)(document)\n",
    "# encoded: sentences to bi-lstm for document encoding \n",
    "b_lstm_doc = \\\n",
    "    Bidirectional(LSTM(128, return_sequences=False, dropout=0.15, recurrent_dropout=0.15, implementation=0))(encoded)\n",
    "\n",
    "output = Dropout(0.3)(b_lstm_doc)\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.3)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(inputs=document, outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if checkpoint:\n",
    "    model.load_weights(checkpoint)\n",
    "\n",
    "check_cb = keras.callbacks.ModelCheckpoint('checkpoints/' + \"checkpoints\" + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                           monitor='val_loss',verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto')\n",
    "#history = LossHistory()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=5,\n",
    "          epochs=5, shuffle=True, callbacks=[earlystop_cb, check_cb, history])\n",
    "\n",
    "# just showing access to the history object\n",
    "#print history.losses\n",
    "#print history.accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
