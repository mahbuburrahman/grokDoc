{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section header classification using characher based RNN. It is based on updated version of tensorflow and cuda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas\n",
    "from sklearn import metrics,cross_validation\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "learn = tf.contrib.learn\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import warnings\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "HIDDEN_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_dir=\"../s3/training_data/train/\"\n",
    "test_dir=\"../s3/training_data/test/\"\n",
    "\n",
    "duplicate_samples=1\n",
    "duplicate_samples_pos=1\n",
    "n_files=1\n",
    "n_epoch=50\n",
    "f= open(\"report_sections_rnn_combine_text_layout_04_17_17_files_\"+str(n_files)+\"_epoch_\"+str(n_epoch)+\".txt\",\"w\")\n",
    "f.write(\"Number of files: \")\n",
    "f.write(str(n_files))\n",
    "f.write(\"\\nNumber of epoch: \")\n",
    "f.write(str(n_epoch))\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate layout features for training and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yield_each_file_json(all_json_objs):\n",
    "    for reader in all_json_objs:\n",
    "        yield reader\n",
    "\n",
    "def yield_csv_file(reader):\n",
    "    for row in reader:\n",
    "        yield row\n",
    "              \n",
    "def generate_layout_features(reader,voc):\n",
    "    \"\"\" Generate layout features which will be concatenated with text fetaures features after \n",
    "        applying characterbased vectorization.\n",
    "    \"\"\"\n",
    "    feature_names = [\"pos_nnp\",\"without_verb_higher_line_space\",\"font_weight\",\"bold_italic\",\"at_least_3_lines_upper\",\"higher_line_space\",'number_dot','text_len_group','seq_number','references_appendix','header_0','header_1','header_2',\"title_case\",\"all_upper\",\"voc\"]\n",
    "    layout_data =[]\n",
    "    rawtext = []\n",
    "    target = []\n",
    "    auxiliary_verb = [\"is\",\"was\",\"were\",\"am\",\"are\",\"may\",\"might\",\"be\",\"will\",\"shall\",\"should\",\"must\",\"need\",\"have\",\"can\",\"could\",\"ought\",\"would\"]\n",
    "    one_letter=list(string.ascii_uppercase)+list(string.ascii_lowercase)\n",
    "    one_letter.append(\"I\")\n",
    "    one_letter.append(\"II\")\n",
    "    one_letter.append(\"III\")\n",
    "    one_letter.append(\"IV\")\n",
    "    one_letter.append(\"V\")\n",
    "    one_letter.append(\"VI\")\n",
    "    one_letter.append(\"VII\")\n",
    "    one_letter.append(\"VIII\")\n",
    "    one_letter.append(\"IX\")\n",
    "    one_letter.append(\"X\")\n",
    "    all_json_objs={}\n",
    "    for row in yield_csv_file(reader):\n",
    "        all_json_objs.setdefault(row['file_name'],[]).append(row)\n",
    "    del reader\n",
    "    \n",
    "    for reader in  yield_each_file_json(all_json_objs):\n",
    "        each_file_json= all_json_objs[reader]\n",
    "        #print \"processing file \"+ reader\n",
    "        #for row in each_file_json:\n",
    "        all_font_weights=[]\n",
    "        all_font_size=[]\n",
    "        avg_font_weight =0.0\n",
    "        avg_font_size =0.0\n",
    "        avg_line_space =0.0\n",
    "        minimum_line_space =100.0\n",
    "        line_index=0\n",
    "        counted_lines=0            \n",
    "        \n",
    "        skip_authors=True\n",
    "        for line in each_file_json:\n",
    "            # Remove contents till abstract. \n",
    "            if re.match(\"^(abstract)\",line[\"text\"].strip(),re.IGNORECASE):\n",
    "                skip_authors=False                \n",
    "            if int(line[\"page-number\"])<2 and skip_authors==True:\n",
    "                continue\n",
    "\n",
    "            all_font_weights.append(float(line['font-weight']))\n",
    "            all_font_size.append(float(line['font_size']))\n",
    "            avg_font_weight += float(line['font-weight'])\n",
    "            avg_font_size += float(line['font_size'])\n",
    "            # line space \n",
    "            if line_index < len(each_file_json)-1:\n",
    "                if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]: \n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>50:\n",
    "                        continue\n",
    "                    avg_line_space += abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))<minimum_line_space:\n",
    "                        minimum_line_space = abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))\n",
    "                    counted_lines += 1\n",
    "\n",
    "            line_index += 1\n",
    "        if counted_lines !=0:\n",
    "            avg_line_space = avg_line_space/counted_lines    \n",
    "        avg_font_size = avg_font_size/len(each_file_json)\n",
    "        avg_font_weight = avg_font_weight/len(each_file_json)\n",
    "        \n",
    "        font_weight_counter = defaultdict(int)\n",
    "        for word in all_font_weights:  \n",
    "            font_weight_counter[word] += 1\n",
    "        font_weight_counter = sorted(font_weight_counter, key = font_weight_counter.get, reverse = True)\n",
    "        \n",
    "        font_size_counter = defaultdict(int)\n",
    "        for word in all_font_size:  \n",
    "            font_size_counter[word] += 1            \n",
    "        font_size_counter = sorted(font_size_counter, key = font_size_counter.get, reverse = True)\n",
    "        \n",
    "        line_index=0\n",
    "        skip_authors=True\n",
    "        for line in each_file_json:\n",
    "            each_element={}\n",
    "            each_element[\"text\"]= line[\"text\"].strip()\n",
    "            # Remove contents till abstract. \n",
    "            if re.match(\"^(abstract)\",line[\"text\"].strip(),re.IGNORECASE):\n",
    "                skip_authors=False                \n",
    "            if int(line[\"page-number\"])<2 and skip_authors==True:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # check line starts with a number or not\n",
    "            if line[\"text\"].decode('utf-8').split(\" \")[0].replace(\".\",\"\").isdigit() or line[\"text\"].decode('utf-8').split(\" \")[0] in one_letter:\n",
    "                if len(line[\"text\"].split())<5:\n",
    "                    each_element[\"text_len_group\"]=1\n",
    "                elif len(line[\"text\"].split())<7:\n",
    "                    each_element[\"text_len_group\"]=2\n",
    "                else:\n",
    "                    each_element[\"text_len_group\"]=3\n",
    "            else:\n",
    "                if len(line[\"text\"].split())<4:\n",
    "                    each_element[\"text_len_group\"]=1\n",
    "                elif len(line[\"text\"].split())<6:\n",
    "                    each_element[\"text_len_group\"]=2\n",
    "                else:\n",
    "                    each_element[\"text_len_group\"]=3   \n",
    "\n",
    "            #if \":\" in line[\"text\"].decode('utf-8'):\n",
    "            if re.match(\"^(references|appendix)\",line[\"text\"],re.IGNORECASE):\n",
    "                each_element[\"references_appendix\"]=1\n",
    "            else:    \n",
    "                each_element[\"references_appendix\"]=0\n",
    "\n",
    "            #if re.match(\"((\\d+|[a-z])\\s?\\.)\",line[\"text\"],re.IGNORECASE):\n",
    "            if re.match(\"^\\d+(\\s|\\.)+(\\d+(\\s|\\.)+)*[a-z]+\",line[\"text\"],re.IGNORECASE) or re.match(\"^[a-z](\\s\\.\\s)\",line[\"text\"],re.IGNORECASE):\n",
    "                each_element[\"number_dot\"]=1\n",
    "            else:\n",
    "                each_element[\"number_dot\"]=0                    \n",
    "\n",
    "            #if re.match(\"((\\d+|(IX|IV|V?I{0,3}))\\s?(\\.|\\))(\\d*))\",line[\"text\"],re.IGNORECASE):\n",
    "            #if re.match(\"(\\d+|(([MDCLXVI])M*(C[MD]|D?C*)(X[CL]|L?X*)(I[XV]|V?I*)))(\\s|\\.|\\))?\\d*\",line[\"text\"],re.IGNORECASE):\n",
    "            if re.match(\"^([a-z]|(IX|IV|V?I{0,3}))(\\.|\\s)\",line[\"text\"],re.IGNORECASE):\n",
    "                each_element[\"seq_number\"]=1\n",
    "            else:\n",
    "                each_element[\"seq_number\"]=0    \n",
    "\n",
    "            # case features\n",
    "            each_element[\"at_least_3_lines_upper\"] = 0\n",
    "            if line[\"text\"].isupper():\n",
    "                each_element[\"all_upper\"]=1\n",
    "                if line_index > 0 and line_index< len(each_file_json)-1:\n",
    "                    if each_file_json[line_index-1][\"text\"].isupper() and each_file_json[line_index+1][\"text\"].isupper():                     \n",
    "                        each_element[\"at_least_3_lines_upper\"]=1   \n",
    "            else:\n",
    "                each_element[\"all_upper\"]=0    \n",
    "\n",
    "            #line[\"text\"]=\"2 Preliminaries and Main Results\"\n",
    "            count_title=0\n",
    "            for word in line[\"text\"].strip().decode('utf-8').split(\" \"):\n",
    "                if word.istitle():\n",
    "                    count_title+=1\n",
    "\n",
    "            # checking the first word as number and then increase title word by one\n",
    "            if line[\"text\"].strip().decode('utf-8').split(\" \")[0].replace(\".\",\"\").isdigit():\n",
    "                count_title+=1\n",
    "\n",
    "            if count_title/float(len(line[\"text\"].strip().decode('utf-8').split(\" \")))>0.50:\n",
    "                each_element[\"title_case\"]=1\n",
    "            else:\n",
    "                each_element[\"title_case\"]=0\n",
    "\n",
    "\n",
    "            verb_flag =0 # no auxiliary verb\n",
    "            for verb in auxiliary_verb:\n",
    "                if verb in line[\"text\"].decode('utf-8').split(\" \"):\n",
    "                    verb_flag=1\n",
    "                    break\n",
    "\n",
    "            each_element[\"without_verb_higher_line_space\"] = 0\n",
    "            if verb_flag == 0:      \n",
    "                if line_index < len(each_file_json)-1 and line_index > 0:              \n",
    "                    if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"] and each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space and abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>minimum_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1\n",
    "                elif line_index > 0:\n",
    "                    if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>avg_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1        \n",
    "                elif line_index < len(each_file_json)-1:\n",
    "                    if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1        \n",
    "\n",
    "            # only line spaceing \n",
    "            each_element[\"higher_line_space\"] = 0\n",
    "            if line_index < len(each_file_json)-1 and line_index > 0:              \n",
    "                if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"] and each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space and abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>minimum_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1\n",
    "            elif line_index > 0:\n",
    "                if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>avg_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1        \n",
    "            elif line_index < len(each_file_json)-1:\n",
    "                if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1        \n",
    "\n",
    "\n",
    "            #if float(line[\"font_size\"])>12.0:\n",
    "            if float(line[\"font_size\"])>font_size_counter[0]:\n",
    "            #if float(line[\"font_size\"])>avg_font_size:    \n",
    "                each_element[\"header_0\"] =1\n",
    "            else:\n",
    "                each_element[\"header_0\"] =0\n",
    "\n",
    "            #if float(line[\"font_size\"])>=12.0 and float(line[\"font-weight\"])>=300.0:\n",
    "            if float(line[\"font_size\"])>=font_size_counter[0] and float(line[\"font-weight\"])>font_weight_counter[0]:\n",
    "            #if float(line[\"font_size\"])>=avg_font_size and float(line[\"font-weight\"])>avg_font_weight:\n",
    "                each_element[\"header_1\"] =1\n",
    "            else:\n",
    "                each_element[\"header_1\"] =0\n",
    "\n",
    "            #if float(line[\"font_size\"]) >=12.0 and \"bold\" in line[\"font-family\"].lower():\n",
    "            if float(line[\"font_size\"]) >= font_size_counter[0] and \"bold\" in line[\"font-family\"].lower():    \n",
    "            #if float(line[\"font_size\"]) >= avg_font_size and \"bold\" in line[\"font-family\"].lower():\n",
    "                each_element[\"header_2\"] =1\n",
    "            else:\n",
    "                each_element[\"header_2\"] =0\n",
    "\n",
    "            if float(line[\"font-weight\"])>font_weight_counter[0]:\n",
    "                each_element[\"font_weight\"] =1\n",
    "            else:\n",
    "                each_element[\"font_weight\"] =0\n",
    "\n",
    "            if \"bold\" in line[\"font-family\"].lower() and \"italic\" in line[\"font-family\"].lower():\n",
    "                each_element[\"bold_italic\"] =1\n",
    "            else:\n",
    "                each_element[\"bold_italic\"] =0\n",
    "\n",
    "\n",
    "            # POS tagging\n",
    "            tokens = nltk.word_tokenize(line[\"text\"].decode('utf-8'))\n",
    "            text = nltk.Text(tokens)\n",
    "            tags = nltk.pos_tag(text) \n",
    "            counts = Counter(tag for word,tag in tags)\n",
    "            total_pos = sum(counts.values())\n",
    "            pos = dict((word, float(count)/total_pos) for word,count in counts.items())\n",
    "\n",
    "            if \"NNP\" in pos.keys() and \"NN\" in pos.keys():\n",
    "                if pos[\"NNP\"] + pos[\"NN\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            elif \"NNP\" in pos.keys():\n",
    "                if pos[\"NNP\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            elif \"NN\" in pos.keys():\n",
    "                if pos[\"NN\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            else:\n",
    "                each_element[\"pos_nnp\"]=0\n",
    "\n",
    "            each_element[\"voc\"]=0                \n",
    "            for w in each_element['text'].split():\n",
    "                if w in voc:\n",
    "                    each_element[\"voc\"]=1\n",
    "                    break\n",
    "            \n",
    "            if int(line['class']) ==0:\n",
    "                target.append(0)\n",
    "            else:\n",
    "                target.append(1)\n",
    "                \n",
    "            rawtext.append(each_element['text'])\n",
    "            layout_data.append([each_element[\"pos_nnp\"],each_element[\"without_verb_higher_line_space\"],each_element[\"font_weight\"],each_element[\"bold_italic\"],each_element[\"at_least_3_lines_upper\"],each_element[\"higher_line_space\"],each_element['number_dot'],each_element['text_len_group'],each_element['seq_number'],each_element['references_appendix'],each_element['header_0'],each_element['header_1'],each_element['header_2'],each_element[\"title_case\"],each_element[\"all_upper\"],each_element[\"voc\"]])\n",
    "            line_index += 1\n",
    "\n",
    "    del all_json_objs\n",
    "    return layout_data,rawtext,target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine layout and text features for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sh_dataset = defaultdict(lambda : None)\n",
    "sh_dataset['target_names'] =['no','yes']\n",
    "sh_dataset['target'] =[]\n",
    "sh_dataset['data'] =[]\n",
    "sh_dataset['layout'] =[]\n",
    "\n",
    "real_dataset = defaultdict(lambda : None)\n",
    "real_dataset['target_names'] =['no','yes']\n",
    "real_dataset['target'] =[]\n",
    "real_dataset['data'] =[]\n",
    "real_dataset['layout'] =[]\n",
    "\n",
    "with open('new_vocabulary.json', 'r') as fp:\n",
    "    voc = json.load(fp)\n",
    "\n",
    "number_process_files=1\n",
    "for ann_file in glob.glob(ann_dir+\"/*.csv\"):\n",
    "    if number_process_files>n_files:\n",
    "        continue\n",
    "    print \"Pocessing input file \", ann_file    \n",
    "    with open(ann_file, 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        layout_featues,raw_text,target = generate_layout_features(reader,voc) # get layout features for each csv file\n",
    "        sh_dataset['target']+=target\n",
    "        sh_dataset['data']+=raw_text\n",
    "        sh_dataset['layout']+=layout_featues\n",
    "    number_process_files+=1\n",
    "\n",
    "number_process_files=1\n",
    "for test_file in glob.glob(test_dir+\"/*.csv\"):\n",
    "    if number_process_files>n_files:\n",
    "        continue\n",
    "    print \"Pocessing input file \", test_file    \n",
    "    with open(test_file, 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        layout_featues,raw_text,target = generate_layout_features(reader,voc) # get layout features for each csv file\n",
    "        real_dataset['target']+=target\n",
    "        real_dataset['data']+=raw_text\n",
    "        real_dataset['layout']+=layout_featues\n",
    "    number_process_files+=1\n",
    "\n",
    "print \"Done data training and test data processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balance positive and ngative samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "training_dataset = defaultdict(lambda : None)\n",
    "training_dataset['target_names'] =['no','yes']\n",
    "training_dataset['target'] =[]\n",
    "training_dataset['data'] =[]\n",
    "training_dataset['layout'] =[]\n",
    "\n",
    "pos_sample= sh_dataset['target'].count(1)\n",
    "count_neg_sample=0\n",
    "for i in range(len(sh_dataset['target'])):\n",
    "    if sh_dataset['target'][i]==1:\n",
    "        training_dataset['data'].append(sh_dataset['data'][i])\n",
    "        training_dataset['target'].append(sh_dataset['target'][i])\n",
    "        training_dataset['layout'].append(sh_dataset['layout'][i])\n",
    "    else:\n",
    "        if count_neg_sample<pos_sample:\n",
    "            training_dataset['data'].append(sh_dataset['data'][i])\n",
    "            training_dataset['target'].append(sh_dataset['target'][i])\n",
    "            training_dataset['layout'].append(sh_dataset['layout'][i])\n",
    "            count_neg_sample+=1\n",
    "\n",
    "# test data\n",
    "test_dataset = defaultdict(lambda : None)\n",
    "test_dataset['target_names'] =['no','yes']\n",
    "test_dataset['target'] =[]\n",
    "test_dataset['data'] =[]\n",
    "test_dataset['layout'] =[]\n",
    "\n",
    "pos_sample= real_dataset['target'].count(1)\n",
    "count_neg_sample=0\n",
    "for i in range(len(real_dataset['target'])):\n",
    "    if real_dataset['target'][i]==1:\n",
    "        test_dataset['data'].append(real_dataset['data'][i])\n",
    "        test_dataset['target'].append(real_dataset['target'][i])\n",
    "        test_dataset['layout'].append(real_dataset['layout'][i])\n",
    "    else:\n",
    "        if count_neg_sample<pos_sample:\n",
    "            test_dataset['data'].append(real_dataset['data'][i])\n",
    "            test_dataset['target'].append(real_dataset['target'][i])\n",
    "            test_dataset['layout'].append(real_dataset['layout'][i])\n",
    "            count_neg_sample+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_dataset_ran = defaultdict(lambda : None)\n",
    "training_dataset_ran['target_names'] =['no','yes']\n",
    "training_dataset_ran['target'] =[]\n",
    "training_dataset_ran['data'] =[]\n",
    "training_dataset_ran['layout'] =[]\n",
    "\n",
    "ran_index= random.sample(range(0, len(training_dataset['data'])), len(training_dataset['data']))\n",
    "for i in ran_index:\n",
    "    training_dataset_ran['data'].append(training_dataset['data'][i])\n",
    "    training_dataset_ran['target'].append(training_dataset['target'][i])\n",
    "    training_dataset_ran['layout'].append(training_dataset['layout'][i])\n",
    "\n",
    "del training_dataset   \n",
    "del sh_dataset\n",
    "del real_dataset\n",
    "\n",
    "print \"Training\"\n",
    "f.write(\"Training\\n\")\n",
    "print \"positive samples \",training_dataset_ran[\"target\"].count(1)\n",
    "f.write(\"positive samples \")\n",
    "f.write(str(training_dataset_ran[\"target\"].count(1)))\n",
    "f.write(\"\\n\")\n",
    "print \"negative samples \",training_dataset_ran[\"target\"].count(0)\n",
    "f.write(\"negative samples \")\n",
    "f.write(str(training_dataset_ran[\"target\"].count(0)))\n",
    "f.write(\"\\n\")\n",
    "print \"test\"\n",
    "f.write(\"Test\\n\")\n",
    "print \"positive samples \",test_dataset[\"target\"].count(1)\n",
    "f.write(\"positive samples \")\n",
    "f.write(str(test_dataset[\"target\"].count(1)))\n",
    "f.write(\"\\n\")\n",
    "print \"negative samples \",test_dataset[\"target\"].count(0)\n",
    "f.write(\"negative samples \")\n",
    "f.write(str(test_dataset[\"target\"].count(0)))\n",
    "f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def char_rnn_model(features, target):\n",
    "    \"\"\"Character level recurrent neural network model to predict classes.\"\"\"\n",
    "    target = tf.one_hot(target, 15, 1, 0)\n",
    "    #byte_list = tf.one_hot(features, 256, 1, 0)\n",
    "    byte_list = tf.cast(tf.one_hot(features, 256, 1, 0), dtype=tf.float32)\n",
    "    byte_list = tf.unstack(byte_list, axis=1)\n",
    "\n",
    "    cell = tf.contrib.rnn.GRUCell(HIDDEN_SIZE)\n",
    "    _, encoding = tf.contrib.rnn.static_rnn(cell, byte_list, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n",
    "    #loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits=logits, onehot_labels=target)\n",
    "\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss,\n",
    "      tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam',\n",
    "      learning_rate=0.001)\n",
    "\n",
    "    return ({\n",
    "      'class': tf.argmax(logits, 1),\n",
    "      'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--test_with_fake_data',\n",
    "    default=False,\n",
    "    help='Test the example code with fake data.',\n",
    "    action='store_true')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare training and testing data for text data\n",
    "\n",
    "x_train,y_train = training_dataset_ran['data'],training_dataset_ran['target']\n",
    "x_test,  y_test = test_dataset['data'], test_dataset['target']\n",
    "\n",
    "\n",
    "# Process vocabulary\n",
    "char_processor = learn.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "x_train_text = np.array(list(char_processor.fit_transform(x_train)))\n",
    "x_test_text = np.array(list(char_processor.transform(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare training and test data for layout features\n",
    "# convert training and test dataset into numpy array\n",
    "x_train_layout = np.array(training_dataset_ran['layout'])\n",
    "x_test_layout = np.array(test_dataset['layout'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Combine layout and text data\n",
    "x_train=np.zeros(shape=(len(x_train_text),x_train_layout.shape[1]+x_train_text.shape[1]),dtype=np.uint8)\n",
    "for i in range(len(x_train_text)):\n",
    "    x_train[i] = np.concatenate([x_train_layout[i],x_train_text[i]])\n",
    "\n",
    "    \n",
    "    \n",
    "x_test=np.zeros(shape=(len(x_test_text),x_test_layout.shape[1]+x_test_text.shape[1]),dtype=np.uint8)\n",
    "for i in range(len(x_test_text)):\n",
    "    x_test[i] = np.concatenate([x_test_layout[i],x_test_text[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "classifier = learn.Estimator(model_fn=char_rnn_model)\n",
    "\n",
    "\n",
    "# Train and predict\n",
    "count=0\n",
    "while count<n_epoch:\n",
    "    classifier.fit(x_train, y_train, steps=1000,batch_size=10)\n",
    "    y_predicted = [\n",
    "          p['class'] for p in classifier.predict(\n",
    "          x_test, as_iterable=True,batch_size=10)\n",
    "    ]\n",
    "    score = metrics.accuracy_score(y_test, y_predicted)\n",
    "    print('Accuracy: {0:f}'.format(score))\n",
    "    f.write('Accuracy: {0:f}'.format(score))\n",
    "    f.write(\"\\n\")\n",
    "    count+=1\n",
    "\n",
    "\n",
    "print \"\\n More details:\"\n",
    "f.write(\"\\n More details:\\n\")\n",
    "predicted = [\n",
    "          p['class'] for p in classifier.predict(\n",
    "          x_test, as_iterable=True,batch_size=10)\n",
    "    ]\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "f.write(str(metrics.classification_report(y_test, predicted)))\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print \"Confusion Matrix\"\n",
    "f.write(\"\\nConfusion Matrix\")\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "print(cm)\n",
    "f.write(str(cm))\n",
    "print \"Done\"\n",
    "f.write(\"\\nDone: See report file for more details result\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
