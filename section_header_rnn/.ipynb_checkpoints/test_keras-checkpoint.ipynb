{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, merge, Convolution2D, MaxPooling2D, Dropout, Activation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model,Sequential\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import numpy as np\n",
    "import skflow\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding, LSTM,Lambda,Conv1D,MaxPooling1D,Bidirectional,TimeDistributed\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "from keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#datafile= \"data_for_weka_all.csv\"\n",
    "datafile1= \"../s3/training_data/train_sample/train.part1.csv\"\n",
    "datafile2= \"../s3/training_data/train/train.part2.csv\"\n",
    "datafile3= \"../s3/training_data/test/test.part501.csv\"\n",
    "\n",
    "duplicate_samples=1\n",
    "duplicate_samples_pos=1\n",
    "\n",
    "checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert section header dataset for rnn neural network\n",
    "\n",
    "# data for training and test where we may duplicate positive samples. We may also split the data \n",
    "# into two set. training and test. This ae for moel tesing. \n",
    "sh_dataset = defaultdict(lambda : None)\n",
    "sh_dataset['target_names'] =['no','yes']\n",
    "sh_dataset['target'] =[]\n",
    "sh_dataset['data'] =[]\n",
    "\n",
    "# real  data is for tesing the model on a different sample set\n",
    "real_dataset = defaultdict(lambda : None)\n",
    "real_dataset['target_names'] =['no','yes']\n",
    "real_dataset['target'] =[]\n",
    "real_dataset['data'] =[]\n",
    "\n",
    "\n",
    "\n",
    "with open(datafile1, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(duplicate_samples_pos):\n",
    "                    sh_dataset['target'].append(1)\n",
    "                    sh_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                sh_dataset['target'].append(0)  \n",
    "                sh_dataset['data'].append(row['text'])\n",
    "with open(datafile2, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(duplicate_samples_pos):\n",
    "                    sh_dataset['target'].append(1)\n",
    "                    sh_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                sh_dataset['target'].append(0)  \n",
    "                sh_dataset['data'].append(row['text'])\n",
    "\n",
    "with open(datafile3, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(1): # as in relad test, we don't need to dupliate positive samples\n",
    "                    real_dataset['target'].append(1)\n",
    "                    real_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                real_dataset['target'].append(0)  \n",
    "                real_dataset['data'].append(row['text'])\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = ''\n",
    "docs = []\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "for cont, sentiment in zip(sh_dataset['data'],sh_dataset['target']):\n",
    "    sentences = cont\n",
    "    docs.append(sentences)\n",
    "    sentiments.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = set(txt)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "max_sentences = 10\n",
    "\n",
    "X = np.ones((len(docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "y = np.array(sentiments)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                X[i, j, (maxlen-1-t)] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('acc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(x, sz=71):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))\n",
    "def binarize_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_length = [5, 3, 3]\n",
    "nb_filter = [196, 196, 256]\n",
    "pool_length = 2\n",
    "\n",
    "in_sentence = Input(shape=(maxlen,), dtype='int64')\n",
    "# binarize function creates a onehot encoding of each character index\n",
    "embedded = Lambda(binarize, output_shape=binarize_outshape)(in_sentence)\n",
    "\n",
    "for i in range(len(nb_filter)):\n",
    "    embedded = Conv1D(nb_filter=nb_filter[i],\n",
    "                            filter_length=filter_length[i],\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            init='glorot_normal',\n",
    "                            subsample_length=1)(embedded)\n",
    "\n",
    "    embedded = Dropout(0.1)(embedded)\n",
    "    embedded = MaxPooling1D(pool_length=pool_length)(embedded)\n",
    "\n",
    "forward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(embedded)\n",
    "backward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(embedded)\n",
    "\n",
    "sent_encode = merge([forward_sent, backward_sent], mode='concat', concat_axis=-1)\n",
    "sent_encode = Dropout(0.3)(sent_encode)\n",
    "\n",
    "encoder = Model(input=in_sentence, output=sent_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
