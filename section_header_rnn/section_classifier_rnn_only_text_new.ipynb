{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take input from line classifier, that is section header classifier. Then it will classify each section \n",
    "header as top level section, subsection and sub subsection header. Finally we will split each section based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas\n",
    "from sklearn import metrics,cross_validation\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "learn = tf.contrib.learn\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "HIDDEN_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_dir=\"../s3/training_data/train/\"\n",
    "test_dir=\"../s3/training_data/test/\"\n",
    "\n",
    "duplicate_samples=1\n",
    "duplicate_samples_pos=1\n",
    "n_files=200\n",
    "n_epoch=50\n",
    "f= open(\"report_sections_top_sub_subsub_rnn_05_12_17_files_\"+str(n_files)+\"_epoch_\"+str(n_epoch)+\".txt\",\"w\")\n",
    "f.write(\"Number of files: \")\n",
    "f.write(str(n_files))\n",
    "f.write(\"\\nNumber of epoch: \")\n",
    "f.write(str(n_epoch))\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sh_dataset = defaultdict(lambda : None)\n",
    "sh_dataset['target_names'] =['top','sub','subsub']\n",
    "sh_dataset['target'] =[]\n",
    "sh_dataset['data'] =[]\n",
    "\n",
    "real_dataset = defaultdict(lambda : None)\n",
    "real_dataset['target_names'] =['top','sub','subsub']\n",
    "real_dataset['target'] =[]\n",
    "real_dataset['data'] =[]\n",
    "\n",
    "number_process_files=1\n",
    "for ann_file in glob.glob(ann_dir+\"/*.csv\"):\n",
    "    #print \"Pocessing input file \", ann_file\n",
    "    if number_process_files>n_files:\n",
    "        continue\n",
    "    with open(ann_file, 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            for i in range(duplicate_samples):\n",
    "                if float(row['class']) == 1:\n",
    "                    for j in range(duplicate_samples_pos):\n",
    "                        sh_dataset['target'].append(1)\n",
    "                        sh_dataset['data'].append(row['text'])\n",
    "                elif float(row['class']) == 2:\n",
    "                    for j in range(duplicate_samples_pos):\n",
    "                        sh_dataset['target'].append(2)  \n",
    "                        sh_dataset['data'].append(row['text'])\n",
    "                elif float(row['class']) == 3:\n",
    "                    for j in range(duplicate_samples_pos):\n",
    "                        sh_dataset['target'].append(3)  \n",
    "                        sh_dataset['data'].append(row['text'])\n",
    "    number_process_files+=1\n",
    "\n",
    "number_process_files=1\n",
    "for test_file in glob.glob(test_dir+\"/*.csv\"):\n",
    "    #print \"Pocessing input file \", test_file\n",
    "    if number_process_files>n_files:\n",
    "        continue\n",
    "    with open(test_file, 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            for i in range(duplicate_samples):\n",
    "                if float(row['class']) == 1:\n",
    "                    real_dataset['target'].append(1)\n",
    "                    real_dataset['data'].append(row['text'])\n",
    "                elif float(row['class']) == 2:\n",
    "                    real_dataset['target'].append(2)  \n",
    "                    real_dataset['data'].append(row['text'])\n",
    "                elif float(row['class']) == 3:\n",
    "                    real_dataset['target'].append(3)  \n",
    "                    real_dataset['data'].append(row['text'])\n",
    "                    \n",
    "    number_process_files+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balance samples for top level, sub and sub-sub section header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "training_dataset = defaultdict(lambda : None)\n",
    "training_dataset['target_names'] =['top','sub','subsub']\n",
    "training_dataset['target'] =[]\n",
    "training_dataset['data'] =[]\n",
    "\n",
    "pos_sample = min(sh_dataset['target'].count(1),sh_dataset['target'].count(2),sh_dataset['target'].count(3))\n",
    "\n",
    "count_top_sample=0\n",
    "count_sub_sample=0\n",
    "count_subsub_sample=0\n",
    "\n",
    "for i in range(len(sh_dataset['target'])):\n",
    "    if sh_dataset['target'][i]==1:\n",
    "        if count_top_sample<pos_sample:\n",
    "            training_dataset['data'].append(sh_dataset['data'][i])\n",
    "            training_dataset['target'].append(sh_dataset['target'][i])\n",
    "            count_top_sample+=1\n",
    "    elif sh_dataset['target'][i]==2:\n",
    "        if count_sub_sample<pos_sample:\n",
    "            training_dataset['data'].append(sh_dataset['data'][i])\n",
    "            training_dataset['target'].append(sh_dataset['target'][i])\n",
    "            count_sub_sample+=1\n",
    "    else:\n",
    "        if count_subsub_sample<pos_sample:\n",
    "            training_dataset['data'].append(sh_dataset['data'][i])\n",
    "            training_dataset['target'].append(sh_dataset['target'][i])\n",
    "            count_subsub_sample+=1\n",
    "\n",
    "# test data\n",
    "test_dataset = defaultdict(lambda : None)\n",
    "test_dataset['target_names'] =['top','sub','subsub']\n",
    "test_dataset['target'] =[]\n",
    "test_dataset['data'] =[]\n",
    "\n",
    "pos_sample = min(real_dataset['target'].count(1),real_dataset['target'].count(2),real_dataset['target'].count(3))\n",
    "\n",
    "count_top_sample=0\n",
    "count_sub_sample=0\n",
    "count_subsub_sample=0\n",
    "\n",
    "for i in range(len(real_dataset['target'])):\n",
    "    if real_dataset['target'][i]==1:\n",
    "        if count_top_sample<pos_sample:\n",
    "            test_dataset['data'].append(real_dataset['data'][i])\n",
    "            test_dataset['target'].append(real_dataset['target'][i])\n",
    "            count_top_sample+=1\n",
    "    elif real_dataset['target'][i]==2:\n",
    "        if count_sub_sample<pos_sample:\n",
    "            test_dataset['data'].append(real_dataset['data'][i])\n",
    "            test_dataset['target'].append(real_dataset['target'][i])\n",
    "            count_sub_sample+=1\n",
    "    else:\n",
    "        if count_subsub_sample<pos_sample:\n",
    "            test_dataset['data'].append(real_dataset['data'][i])\n",
    "            test_dataset['target'].append(real_dataset['target'][i])\n",
    "            count_subsub_sample+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_dataset_ran = defaultdict(lambda : None)\n",
    "training_dataset_ran['target_names'] =['top','sub','subsub']\n",
    "training_dataset_ran['target'] =[]\n",
    "training_dataset_ran['data'] =[]\n",
    "\n",
    "ran_index= random.sample(range(0, len(training_dataset['data'])), len(training_dataset['data']))\n",
    "for i in ran_index:\n",
    "    training_dataset_ran['data'].append(training_dataset['data'][i])\n",
    "    training_dataset_ran['target'].append(training_dataset['target'][i])\n",
    "\n",
    "del training_dataset   \n",
    "del sh_dataset\n",
    "del real_dataset\n",
    "\n",
    "print \"Training\"\n",
    "f.write(\"Training\\n\")\n",
    "print \"Top samples \",training_dataset_ran[\"target\"].count(1)\n",
    "f.write(\"Top samples \")\n",
    "f.write(str(training_dataset_ran[\"target\"].count(1)))\n",
    "f.write(\"\\n\")\n",
    "print \"Sub samples \",training_dataset_ran[\"target\"].count(2)\n",
    "f.write(\"Sub samples \")\n",
    "f.write(str(training_dataset_ran[\"target\"].count(2)))\n",
    "f.write(\"\\n\")\n",
    "print \"Subsub samples \",training_dataset_ran[\"target\"].count(3)\n",
    "f.write(\"Subsub samples \")\n",
    "f.write(str(training_dataset_ran[\"target\"].count(3)))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "print \"test\"\n",
    "f.write(\"Test\\n\")\n",
    "print \"Top samples \",test_dataset[\"target\"].count(1)\n",
    "f.write(\"Top samples \")\n",
    "f.write(str(test_dataset[\"target\"].count(1)))\n",
    "f.write(\"\\n\")\n",
    "print \"Sub samples \",test_dataset[\"target\"].count(2)\n",
    "f.write(\"Sub samples \")\n",
    "f.write(str(test_dataset[\"target\"].count(2)))\n",
    "f.write(\"\\n\")\n",
    "print \"Subsub samples \",test_dataset[\"target\"].count(3)\n",
    "f.write(\"Subsub samples \")\n",
    "f.write(str(test_dataset[\"target\"].count(3)))\n",
    "f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char_rnn_model(features, target):\n",
    "    \"\"\"Character level recurrent neural network model to predict classes.\"\"\"\n",
    "    target = tf.one_hot(target, 15, 1, 0)\n",
    "    #byte_list = tf.one_hot(features, 256, 1, 0)\n",
    "    byte_list = tf.cast(tf.one_hot(features, 256, 1, 0), dtype=tf.float32)\n",
    "    byte_list = tf.unstack(byte_list, axis=1)\n",
    "\n",
    "    cell = tf.contrib.rnn.GRUCell(HIDDEN_SIZE)\n",
    "    _, encoding = tf.contrib.rnn.static_rnn(cell, byte_list, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n",
    "    #loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits=logits, onehot_labels=target)\n",
    "\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss,\n",
    "      tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam',\n",
    "      learning_rate=0.001)\n",
    "\n",
    "    return ({\n",
    "      'class': tf.argmax(logits, 1),\n",
    "      'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--test_with_fake_data',\n",
    "    default=False,\n",
    "    help='Test the example code with fake data.',\n",
    "    action='store_true')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare training and testing data\n",
    "\n",
    "x_train,y_train = training_dataset_ran['data'],training_dataset_ran['target']\n",
    "x_test,  y_test = test_dataset['data'], test_dataset['target']\n",
    "\n",
    "#dbpedia = learn.datasets.load_dataset('dbpedia', test_with_fake_data=FLAGS.test_with_fake_data)\n",
    "#x_train = pandas.DataFrame(dbpedia.train.data)[1]\n",
    "#y_train = pandas.Series(dbpedia.train.target)\n",
    "#x_test = pandas.DataFrame(dbpedia.test.data)[1]\n",
    "\n",
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "\n",
    "# Process vocabulary\n",
    "char_processor = learn.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "x_train = np.array(list(char_processor.fit_transform(x_train)))\n",
    "x_test = np.array(list(char_processor.transform(x_test)))\n",
    "\n",
    "# Build model\n",
    "classifier = learn.Estimator(model_fn=char_rnn_model,model_dir=\"sections_char_rnn\")\n",
    "\n",
    "\n",
    "# Train and predict\n",
    "count=0\n",
    "while count<n_epoch:\n",
    "    print \"Number of Epoch\", count\n",
    "    classifier.fit(x_train, y_train, steps=1000,batch_size=10)\n",
    "    y_predicted = [\n",
    "          p['class'] for p in classifier.predict(\n",
    "          x_test, as_iterable=True,batch_size=10)\n",
    "    ]\n",
    "    score = metrics.accuracy_score(y_test, y_predicted)\n",
    "    print('Accuracy: {0:f}'.format(score))\n",
    "    f.write('Accuracy: {0:f}'.format(score))\n",
    "    f.write(\"\\n\")\n",
    "    count+=1\n",
    "\n",
    "\n",
    "print \"\\n More details:\"\n",
    "f.write(\"\\n More details:\\n\")\n",
    "predicted = [\n",
    "          p['class'] for p in classifier.predict(\n",
    "          x_test, as_iterable=True,batch_size=10)\n",
    "    ]\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "f.write(str(metrics.classification_report(y_test, predicted)))\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print \"Confusion Matrix\"\n",
    "f.write(\"\\nConfusion Matrix\")\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "print(cm)\n",
    "f.write(str(cm))\n",
    "print \"Done\"\n",
    "f.write(\"\\nDone: See report file for more details result\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
