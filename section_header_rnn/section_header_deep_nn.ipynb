{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing a deep neural network for section header dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics, cross_validation\n",
    "import tensorflow as tf\n",
    "import skflow\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#datafile= \"data_for_weka_all.csv\"\n",
    "datafile= \"data_for_weka.csv\"\n",
    "acceptance_file= \"acceptance_test_data_for_weka_03_02_16.csv\"\n",
    "duplicate_samples=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert section header dataset for ddep neural network\n",
    "\n",
    "sh_dataset = defaultdict(lambda : None)\n",
    "sh_dataset['target_names'] =['yes','no']\n",
    "sh_dataset['feature_names'] = ['pos_nnp', 'number_dot', 'header_2', 'seq_number', 'all_upper', 'header_1', 'at_least_3_lines_upper', 'text_len_group', 'font_weight', 'higher_line_space', 'header_0', 'bold_italic', 'colon', 'title_case', 'without_verb_higher_line_space']\n",
    "sh_dataset['target'] =[]\n",
    "sh_dataset['data'] =[]\n",
    "\n",
    "with open(datafile, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            sh_dataset['target'].append(int(row['class']))\n",
    "            sh_dataset['data'].append([int(row['pos_nnp']), int(row['number_dot']), int(row['header_2']), int(row['seq_number']), int(row['all_upper']), int(row['header_1']), int(row['at_least_3_lines_upper']), int(row['text_len_group']), int(row['font_weight']), int(row['higher_line_space']), int(row['header_0']), int(row['bold_italic']), int(row['colon']), int(row['title_case']), int(row['without_verb_higher_line_space'])])\n",
    "\n",
    "# for acceptance test\n",
    "sh_acceptance = defaultdict(lambda : None)\n",
    "sh_acceptance['target_names'] =['yes','no']\n",
    "sh_acceptance['feature_names'] = ['pos_nnp', 'number_dot', 'header_2', 'seq_number', 'all_upper', 'header_1', 'at_least_3_lines_upper', 'text_len_group', 'font_weight', 'higher_line_space', 'header_0', 'bold_italic', 'colon', 'title_case', 'without_verb_higher_line_space']\n",
    "sh_acceptance['target'] =[]\n",
    "sh_acceptance['data'] =[]\n",
    "\n",
    "with open(acceptance_file, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        sh_acceptance['target'].append(int(row['class']))\n",
    "        sh_acceptance['data'].append([int(row['pos_nnp']), int(row['number_dot']), int(row['header_2']), int(row['seq_number']), int(row['all_upper']), int(row['header_1']), int(row['at_least_3_lines_upper']), int(row['text_len_group']), int(row['font_weight']), int(row['higher_line_space']), int(row['header_0']), int(row['bold_italic']), int(row['colon']), int(row['title_case']), int(row['without_verb_higher_line_space'])])\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples:  476\n",
      "Training samples:  380\n",
      "Test samples:  96\n",
      "Total negative samples:  238\n",
      "Total positive samples:  238\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(sh_dataset['data'], sh_dataset['target'],\n",
    "    test_size=0.2, random_state=42)\n",
    "\n",
    "print \"Total samples: \",len(sh_dataset['data'])\n",
    "print \"Training samples: \",len(X_train)\n",
    "print \"Test samples: \",len(X_test)\n",
    "\n",
    "\n",
    "print \"Total negative samples: \",y_train.count(0)+y_test.count(0)\n",
    "print \"Total positive samples: \",y_train.count(1)+y_test.count(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train= np.array(X_train,dtype='float64')\n",
    "X_test= np.array(X_test,dtype='float64')\n",
    "y_train= np.array(y_train,dtype='float64')\n",
    "y_test= np.array(y_test,dtype='float64')\n",
    "\n",
    "\n",
    "# acceptance test data converting to numpy array \n",
    "X_acceptance= np.array(sh_acceptance['data'],dtype='float64')\n",
    "y_acceptance= np.array(sh_acceptance['target'],dtype='float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  1. ...,  0.  1.  0.]\n",
      " [ 1.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]] (380, 15)\n"
     ]
    }
   ],
   "source": [
    "print X_train, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following will create 3 layers of fully connected units with 10, 20 and 10 hidden units respectively, with default Rectified linear unit activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build 3 layer DNN with 10, 20, 10 units respecitvely.\n",
    "classifier = skflow.TensorFlowDNNClassifier(hidden_units=[100, 100, 200, 100, 100],\n",
    "    n_classes=2, steps=10000,learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #8, avg. train loss: 0.61114\n",
      "Step #200, epoch #16, avg. train loss: 0.38963\n",
      "Step #300, epoch #25, avg. train loss: 0.31890\n",
      "Step #400, epoch #33, avg. train loss: 0.28694\n",
      "Step #500, epoch #41, avg. train loss: 0.26736\n",
      "Step #600, epoch #50, avg. train loss: 0.24907\n",
      "Step #700, epoch #58, avg. train loss: 0.23929\n",
      "Step #800, epoch #66, avg. train loss: 0.22402\n",
      "Step #900, epoch #75, avg. train loss: 0.21772\n",
      "Step #1000, epoch #83, avg. train loss: 0.20859\n",
      "Step #1100, epoch #91, avg. train loss: 0.20242\n",
      "Step #1200, epoch #100, avg. train loss: 0.19384\n",
      "Step #1300, epoch #108, avg. train loss: 0.18737\n",
      "Step #1400, epoch #116, avg. train loss: 0.17752\n",
      "Step #1500, epoch #125, avg. train loss: 0.17211\n",
      "Step #1600, epoch #133, avg. train loss: 0.16177\n",
      "Step #1700, epoch #141, avg. train loss: 0.16331\n",
      "Step #1800, epoch #150, avg. train loss: 0.15031\n",
      "Step #1900, epoch #158, avg. train loss: 0.14823\n",
      "Step #2000, epoch #166, avg. train loss: 0.14174\n",
      "Step #2100, epoch #175, avg. train loss: 0.13704\n",
      "Step #2200, epoch #183, avg. train loss: 0.13358\n",
      "Step #2300, epoch #191, avg. train loss: 0.13001\n",
      "Step #2400, epoch #200, avg. train loss: 0.12312\n",
      "Step #2500, epoch #208, avg. train loss: 0.11921\n",
      "Step #2600, epoch #216, avg. train loss: 0.11984\n",
      "Step #2700, epoch #225, avg. train loss: 0.11831\n",
      "Step #2800, epoch #233, avg. train loss: 0.10939\n",
      "Step #2900, epoch #241, avg. train loss: 0.10879\n",
      "Step #3000, epoch #250, avg. train loss: 0.10502\n",
      "Step #3100, epoch #258, avg. train loss: 0.09885\n",
      "Step #3200, epoch #266, avg. train loss: 0.09886\n",
      "Step #3300, epoch #275, avg. train loss: 0.09568\n",
      "Step #3400, epoch #283, avg. train loss: 0.09382\n",
      "Step #3500, epoch #291, avg. train loss: 0.09328\n",
      "Step #3600, epoch #300, avg. train loss: 0.08972\n",
      "Step #3700, epoch #308, avg. train loss: 0.09650\n",
      "Step #3800, epoch #316, avg. train loss: 0.09248\n",
      "Step #3900, epoch #325, avg. train loss: 0.08998\n",
      "Step #4000, epoch #333, avg. train loss: 0.08514\n",
      "Step #4100, epoch #341, avg. train loss: 0.08726\n",
      "Step #4200, epoch #350, avg. train loss: 0.09011\n",
      "Step #4300, epoch #358, avg. train loss: 0.08237\n",
      "Step #4400, epoch #366, avg. train loss: 0.08375\n",
      "Step #4500, epoch #375, avg. train loss: 0.08303\n",
      "Step #4600, epoch #383, avg. train loss: 0.08213\n",
      "Step #4700, epoch #391, avg. train loss: 0.08042\n",
      "Step #4800, epoch #400, avg. train loss: 0.08820\n",
      "Step #4900, epoch #408, avg. train loss: 0.08146\n",
      "Step #5000, epoch #416, avg. train loss: 0.08248\n",
      "Step #5100, epoch #425, avg. train loss: 0.07955\n",
      "Step #5200, epoch #433, avg. train loss: 0.07902\n",
      "Step #5300, epoch #441, avg. train loss: 0.07649\n",
      "Step #5400, epoch #450, avg. train loss: 0.08052\n",
      "Step #5500, epoch #458, avg. train loss: 0.07388\n",
      "Step #5600, epoch #466, avg. train loss: 0.08054\n",
      "Step #5700, epoch #475, avg. train loss: 0.07655\n",
      "Step #5800, epoch #483, avg. train loss: 0.07537\n",
      "Step #5900, epoch #491, avg. train loss: 0.07517\n",
      "Step #6000, epoch #500, avg. train loss: 0.07544\n",
      "Step #6100, epoch #508, avg. train loss: 0.07396\n",
      "Step #6200, epoch #516, avg. train loss: 0.07471\n",
      "Step #6300, epoch #525, avg. train loss: 0.07858\n",
      "Step #6400, epoch #533, avg. train loss: 0.07640\n",
      "Step #6500, epoch #541, avg. train loss: 0.07379\n",
      "Step #6600, epoch #550, avg. train loss: 0.07184\n",
      "Step #6700, epoch #558, avg. train loss: 0.07086\n",
      "Step #6800, epoch #566, avg. train loss: 0.07122\n",
      "Step #6900, epoch #575, avg. train loss: 0.07575\n",
      "Step #7000, epoch #583, avg. train loss: 0.07060\n",
      "Step #7100, epoch #591, avg. train loss: 0.07557\n",
      "Step #7200, epoch #600, avg. train loss: 0.07561\n",
      "Step #7300, epoch #608, avg. train loss: 0.06957\n",
      "Step #7400, epoch #616, avg. train loss: 0.07368\n",
      "Step #7500, epoch #625, avg. train loss: 0.07124\n",
      "Step #7600, epoch #633, avg. train loss: 0.06928\n",
      "Step #7700, epoch #641, avg. train loss: 0.07254\n",
      "Step #7800, epoch #650, avg. train loss: 0.07013\n",
      "Step #7900, epoch #658, avg. train loss: 0.07158\n",
      "Step #8000, epoch #666, avg. train loss: 0.06863\n",
      "Step #8100, epoch #675, avg. train loss: 0.07190\n",
      "Step #8200, epoch #683, avg. train loss: 0.07190\n",
      "Step #8300, epoch #691, avg. train loss: 0.07154\n",
      "Step #8400, epoch #700, avg. train loss: 0.07217\n",
      "Step #8500, epoch #708, avg. train loss: 0.07024\n",
      "Step #8600, epoch #716, avg. train loss: 0.07325\n",
      "Step #8700, epoch #725, avg. train loss: 0.07377\n",
      "Step #8800, epoch #733, avg. train loss: 0.07095\n",
      "Step #8900, epoch #741, avg. train loss: 0.06934\n",
      "Step #9000, epoch #750, avg. train loss: 0.07064\n",
      "Step #9100, epoch #758, avg. train loss: 0.06944\n",
      "Step #9200, epoch #766, avg. train loss: 0.07018\n",
      "Step #9300, epoch #775, avg. train loss: 0.06848\n",
      "Step #9400, epoch #783, avg. train loss: 0.06833\n",
      "Step #9500, epoch #791, avg. train loss: 0.07225\n",
      "Step #9600, epoch #800, avg. train loss: 0.06802\n",
      "Step #9700, epoch #808, avg. train loss: 0.06860\n",
      "Step #9800, epoch #816, avg. train loss: 0.06915\n",
      "Step #9900, epoch #825, avg. train loss: 0.07061\n",
      "Step #10000, epoch #833, avg. train loss: 0.07076\n",
      "Accuracy: 0.916667\n",
      "\n",
      "\n",
      "More details:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.84      0.90        44\n",
      "        1.0       0.88      0.98      0.93        52\n",
      "\n",
      "avg / total       0.92      0.92      0.92        96\n",
      "\n",
      "Confusion Matrix\n",
      "[[37  7]\n",
      " [ 1 51]]\n",
      "\n",
      "\n",
      "For Acceptance test:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.87      0.91       350\n",
      "        1.0       0.33      0.61      0.43        38\n",
      "\n",
      "avg / total       0.89      0.84      0.86       388\n",
      "\n",
      "Confusion Matrix\n",
      "[[303  47]\n",
      " [ 15  23]]\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict based on training and test data.\n",
    "classifier.fit(X_train, y_train,logdir='result_dnn_section_header')\n",
    "score = metrics.accuracy_score(y_test, classifier.predict(X_test))\n",
    "print('Accuracy: {0:f}'.format(score))\n",
    "\n",
    "print \"\\n\\nMore details:\"\n",
    "predicted = classifier.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print \"Confusion Matrix\"\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "# Predicting based on acceptance dataset\n",
    "print \"\\n\\nFor Acceptance test:\"\n",
    "predicted = classifier.predict(X_acceptance)\n",
    "print(metrics.classification_report(y_acceptance, predicted))\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print \"Confusion Matrix\"\n",
    "cm = metrics.confusion_matrix(y_acceptance, predicted)\n",
    "print(cm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Process input data for classifier. This will generate necessary fields for the deep learning classifier. \n",
    "This will be used for testing the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bunch(dict):\n",
    "    \"\"\"Container object for datasets: dictionary-like object that\n",
    "        exposes its keys as attributes.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        dict.__init__(self, kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "def text_delexicalization(text):\n",
    "    \"\"\"delexicalization of each text string\n",
    "    \"\"\"\n",
    "    regular_num = \"#number \"\n",
    "    pattern_reg = re.compile('^(\\d+(\\.\\d+)*(\\.)?)|([a-z]+\\.\\s)', re.IGNORECASE)\n",
    "    rep_text= pattern_reg.sub(regular_num,text)\n",
    "    return rep_text\n",
    "\n",
    "def generate_dataset(ann_file):\n",
    "    target = []\n",
    "    target_names = ['no','yes']\n",
    "    feature_names = ['pos_nnp', 'number_dot', 'header_2', 'seq_number', 'all_upper', 'header_1', 'at_least_3_lines_upper', 'text_len_group', 'font_weight', 'higher_line_space', 'header_0', 'bold_italic', 'colon', 'title_case', 'without_verb_higher_line_space']\n",
    "    rawtext = []\n",
    "    no_delex_rawtext =[]\n",
    "    data =[]\n",
    "    file_names=[]\n",
    "    auxiliary_verb = [\"is\",\"was\",\"were\",\"am\",\"are\",\"may\",\"might\",\"be\",\"will\",\"shall\",\"should\",\"must\",\"need\",\"have\",\"can\",\"could\",\"ought\",\"would\"]\n",
    "            \n",
    "    all_json_objs={}\n",
    "    with open(ann_file, 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            all_json_objs.setdefault(row['file_name'],[]).append(row)\n",
    "    \n",
    "    \n",
    "    for reader in all_json_objs:\n",
    "        each_file_json= all_json_objs[reader]\n",
    "        print \"processing file \"+ reader\n",
    "        all_font_weights=[]\n",
    "        all_font_size=[]\n",
    "        avg_font_weight =0.0\n",
    "        avg_font_size =0.0\n",
    "        avg_line_space =0.0\n",
    "        minimum_line_space =100.0\n",
    "        line_index=0\n",
    "        counted_lines=0            \n",
    "        \n",
    "        for line in each_file_json:\n",
    "            all_font_weights.append(line['font-weight'])\n",
    "            all_font_size.append(line['font_size'])\n",
    "            avg_font_weight += float(line['font-weight'])\n",
    "            avg_font_size += float(line['font_size'])\n",
    "            # line space \n",
    "            if line_index < len(each_file_json)-1:\n",
    "                if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]: \n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>50:\n",
    "                        continue\n",
    "                    avg_line_space += abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))<minimum_line_space:\n",
    "                        minimum_line_space = abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))\n",
    "                    counted_lines += 1\n",
    "            \n",
    "            line_index += 1\n",
    "        if counted_lines !=0:\n",
    "            avg_line_space = avg_line_space/counted_lines    \n",
    "        avg_font_size = avg_font_size/len(each_file_json)\n",
    "        avg_font_weight = avg_font_weight/len(each_file_json)\n",
    "        \n",
    "        font_weight_counter = defaultdict(int)\n",
    "        for word in all_font_weights:  \n",
    "            font_weight_counter[word] += 1\n",
    "        font_weight_counter = sorted(font_weight_counter, key = font_weight_counter.get, reverse = True)\n",
    "        \n",
    "        font_size_counter = defaultdict(int)\n",
    "        for word in all_font_size:  \n",
    "            font_size_counter[word] += 1            \n",
    "        font_size_counter = sorted(font_size_counter, key = font_size_counter.get, reverse = True)\n",
    "        \n",
    "        line_index=0\n",
    "        for line in each_file_json:\n",
    "            each_element={}\n",
    "            \n",
    "            each_element[\"text\"]= line[\"text\"]\n",
    "            if len(line[\"text\"].split())<4:\n",
    "                each_element[\"text_len_group\"]=1\n",
    "            elif len(line[\"text\"].split())<6:\n",
    "                each_element[\"text_len_group\"]=2\n",
    "            else:\n",
    "                each_element[\"text_len_group\"]=3\n",
    "            \n",
    "            if \":\" in line[\"text\"].decode('utf-8'):\n",
    "                each_element[\"colon\"]=1\n",
    "            else:    \n",
    "                each_element[\"colon\"]=0\n",
    "                \n",
    "            if re.match(\"((\\d+|[a-z])\\s?\\.)\",line[\"text\"],re.IGNORECASE):\n",
    "                each_element[\"number_dot\"]=1\n",
    "            else:\n",
    "                each_element[\"number_dot\"]=0                    \n",
    "            \n",
    "            #if re.match(\"((\\d+|(IX|IV|V?I{0,3}))\\s?(\\.|\\))(\\d*))\",line[\"text\"],re.IGNORECASE):\n",
    "            if re.match(\"(\\d+|(([MDCLXVI])M*(C[MD]|D?C*)(X[CL]|L?X*)(I[XV]|V?I*)))(\\s|\\.|\\))?\\d*\",line[\"text\"],re.IGNORECASE):\n",
    "                each_element[\"seq_number\"]=1\n",
    "            else:\n",
    "                each_element[\"seq_number\"]=0    \n",
    "                \n",
    "            # case features\n",
    "            each_element[\"at_least_3_lines_upper\"] = 0\n",
    "            if line[\"text\"].isupper():\n",
    "                each_element[\"all_upper\"]=1\n",
    "                if line_index > 0 and line_index< len(each_file_json)-1:\n",
    "                    if each_file_json[line_index-1][\"text\"].isupper() and each_file_json[line_index+1][\"text\"].isupper():                     \n",
    "                        each_element[\"at_least_3_lines_upper\"]=1   \n",
    "            else:\n",
    "                each_element[\"all_upper\"]=0    \n",
    "            \n",
    "            count_title=0\n",
    "            for word in line[\"text\"].decode('utf-8').split(\" \"):\n",
    "                if word.istitle():\n",
    "                    count_title+=1\n",
    "            \n",
    "            if count_title/float(len(line[\"text\"].decode('utf-8').split(\" \")))>0.50:\n",
    "                each_element[\"title_case\"]=1\n",
    "            else:\n",
    "                each_element[\"title_case\"]=0\n",
    "                \n",
    "            \n",
    "            verb_flag =0 # no auxiliary verb\n",
    "            for verb in auxiliary_verb:\n",
    "                if verb in line[\"text\"].decode('utf-8').split(\" \"):\n",
    "                    verb_flag=1\n",
    "                    break\n",
    "                \n",
    "            each_element[\"without_verb_higher_line_space\"] = 0\n",
    "            if verb_flag == 0:      \n",
    "                if line_index < len(each_file_json)-1 and line_index > 0:              \n",
    "                    if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"] and each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space and abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>minimum_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1\n",
    "                elif line_index > 0:\n",
    "                    if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>avg_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1        \n",
    "                elif line_index < len(each_file_json)-1:\n",
    "                    if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1        \n",
    "                            \n",
    "            # only line spaceing \n",
    "            each_element[\"higher_line_space\"] = 0\n",
    "            if line_index < len(each_file_json)-1 and line_index > 0:              \n",
    "                if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"] and each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space and abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>minimum_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1\n",
    "            elif line_index > 0:\n",
    "                if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>avg_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1        \n",
    "            elif line_index < len(each_file_json)-1:\n",
    "                if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1        \n",
    "                        \n",
    "            \n",
    "                                 \n",
    "            #if line[\"font_size\"]>12:\n",
    "            if line[\"font_size\"]>font_size_counter[0]:\n",
    "            #if line[\"font_size\"]>avg_font_size:    \n",
    "                each_element[\"header_0\"] =1\n",
    "            else:\n",
    "                each_element[\"header_0\"] =0\n",
    "            \n",
    "            #if line[\"font_size\"]>=12 and line[\"font-weight\"]>=300:\n",
    "            if line[\"font_size\"]>=font_size_counter[0] and line[\"font-weight\"]>font_weight_counter[0]:\n",
    "            #if line[\"font_size\"]>=avg_font_size and line[\"font-weight\"]>avg_font_weight:\n",
    "                each_element[\"header_1\"] =1\n",
    "            else:\n",
    "                each_element[\"header_1\"] =0\n",
    "                    \n",
    "            #if line[\"font_size\"] >=12.0 and \"bold\" in line[\"font-family\"].lower():\n",
    "            if line[\"font_size\"] >= font_size_counter[0] and \"bold\" in line[\"font-family\"].lower():    \n",
    "            #if line[\"font_size\"] >= avg_font_size and \"bold\" in line[\"font-family\"].lower():\n",
    "                each_element[\"header_2\"] =1\n",
    "            else:\n",
    "                each_element[\"header_2\"] =0\n",
    "            \n",
    "            if line[\"font-weight\"]>font_weight_counter[0]:\n",
    "                each_element[\"font_weight\"] =1\n",
    "            else:\n",
    "                each_element[\"font_weight\"] =0\n",
    "                    \n",
    "            if \"bold\" in line[\"font-family\"].lower() and \"italic\" in line[\"font-family\"].lower():\n",
    "                each_element[\"bold_italic\"] =1\n",
    "            else:\n",
    "                each_element[\"bold_italic\"] =0\n",
    "                                    \n",
    "             \n",
    "            # POS tagging\n",
    "            tokens = nltk.word_tokenize(line[\"text\"].decode('utf-8'))\n",
    "            text = nltk.Text(tokens)\n",
    "            tags = nltk.pos_tag(text) \n",
    "            counts = Counter(tag for word,tag in tags)\n",
    "            total_pos = sum(counts.values())\n",
    "            pos = dict((word, float(count)/total_pos) for word,count in counts.items())\n",
    "            \n",
    "            if \"NNP\" in pos.keys() and \"NN\" in pos.keys():\n",
    "                if pos[\"NNP\"] + pos[\"NN\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            elif \"NNP\" in pos.keys():\n",
    "                if pos[\"NNP\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            elif \"NN\" in pos.keys():\n",
    "                if pos[\"NN\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            else:\n",
    "                each_element[\"pos_nnp\"]=0\n",
    "                    \n",
    "                            \n",
    "            if line['class'] ==\"yes\":\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "            data.append([each_element[\"pos_nnp\"],each_element['number_dot'],each_element[\"header_2\"],each_element[\"seq_number\"],each_element[\"all_upper\"],each_element[\"header_1\"],each_element[\"at_least_3_lines_upper\"],each_element['text_len_group'],each_element['font_weight'],each_element['higher_line_space'],each_element['header_0'],each_element['bold_italic'],each_element['colon'],each_element[\"title_case\"],each_element[\"without_verb_higher_line_space\"]])\n",
    "            rawtext.append(text_delexicalization(each_element['text']))\n",
    "            no_delex_rawtext.append(each_element['text'])\n",
    "            file_names.append(reader)\n",
    "            line_index += 1\n",
    "            \n",
    "    return Bunch(data=data, feature_names=feature_names,target_names=target_names,target=target,rawtext=rawtext,no_delex_rawtext=no_delex_rawtext,filenames=file_names)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file 47c96cb39cc465d14c3cf90f7cae0014f1cc285a.file.pdf.tetml\n",
      "processing file 565ad0cb1ff1baa09c498f981f697b1472c9f118.pdf.tetml\n",
      "processing file bb1dfc701ed1df96de2f0b46cecd65c181007e68.file.pdf.tetml\n",
      "processing file 834851bfb54f2b09aae586450df167ec4cca66e0.file.pdf.tetml\n",
      "processing file 9cb270020c595cd07108750c9321bd6123dba8d9.pdf.tetml\n",
      "processing file b2fb634f43889e936ad39ce5e4562ceff84e5604.file.pdf.tetml\n",
      "processing file 151d93b34c6fc7b663d88e6ecf585f6e0054921b.pdf.tetml\n",
      "processing file 1392203bd3e5ecc516bc30fb53dc55c672d53179.pdf.tetml\n",
      "processing file 51c69584da480798a6218f47d8fa18339d561173.pdf.tetml\n"
     ]
    }
   ],
   "source": [
    "test_dataset = generate_dataset(\"testset_acrobat_section_header.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length:  15\n",
      "Feature names:  ['pos_nnp', 'number_dot', 'header_2', 'seq_number', 'all_upper', 'header_1', 'at_least_3_lines_upper', 'text_len_group', 'font_weight', 'higher_line_space', 'header_0', 'bold_italic', 'colon', 'title_case', 'without_verb_higher_line_space']\n",
      "\n",
      "\n",
      "Testing the classifier:\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print \"Feature length: \",len(test_dataset.data[0])\n",
    "print \"Feature names: \",test_dataset.feature_names\n",
    "\n",
    "# for test data \n",
    "sh_test = defaultdict(lambda : None)\n",
    "sh_test['target_names'] =['no','yes']\n",
    "sh_test['feature_names'] = ['pos_nnp', 'number_dot', 'header_2', 'seq_number', 'all_upper', 'header_1', 'at_least_3_lines_upper', 'text_len_group', 'font_weight', 'higher_line_space', 'header_0', 'bold_italic', 'colon', 'title_case', 'without_verb_higher_line_space']\n",
    "sh_test['target'] =[]\n",
    "sh_test['data'] =[]\n",
    "\n",
    "for row in test_dataset.data:\n",
    "    #sh_test['target'].append(int(row['class']))\n",
    "    sh_test['data'].append(row)\n",
    "\n",
    "\n",
    "    \n",
    "# test data converting to numpy array \n",
    "X_test= np.array(sh_test['data'],dtype='float64')\n",
    "\n",
    "# Predicting based on acceptance dataset\n",
    "print \"\\n\\nTesting the classifier:\"\n",
    "predicted = classifier.predict(X_test)\n",
    "out_file = codecs.open(\"result_section_header_deeplearning.txt\", \"w\",encoding=\"utf-8\")\n",
    "unique_file_list=[]\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i] ==1:\n",
    "        if test_dataset.filenames[i].split(\".tetml\")[0] not in unique_file_list:\n",
    "            unique_file_list.append(test_dataset.filenames[i].split(\".tetml\")[0])\n",
    "            out_file.write(\"\\n\\n\")\n",
    "            out_file.write(test_dataset.filenames[i].split(\".tetml\")[0])\n",
    "            out_file.write(\"\\n\")\n",
    "            out_file.write(\"======================================================\")\n",
    "            out_file.write(\"\\n\")\n",
    "            out_file.write(test_dataset.no_delex_rawtext[i].decode('utf-8'))\n",
    "            out_file.write(\"\\n\")\n",
    "        else:\n",
    "            out_file.write(test_dataset.no_delex_rawtext[i].decode('utf-8'))\n",
    "            out_file.write(\"\\n\")\n",
    "\n",
    "out_file.close()                \n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
