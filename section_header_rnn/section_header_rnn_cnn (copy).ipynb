{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Developing a deep neural network for section header dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics, cross_validation\n",
    "import tensorflow as tf\n",
    "import skflow\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import codecs\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#datafile= \"data_for_weka_all.csv\"\n",
    "datafile1= \"../s3/training_data/train/train.part1.csv\"\n",
    "datafile2= \"../s3/training_data/train/train.part2.csv\"\n",
    "datafile3= \"../s3/training_data/test/test.part501.csv\"\n",
    "\n",
    "duplicate_samples=1\n",
    "duplicate_samples_pos=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert section header dataset for rnn neural network\n",
    "\n",
    "sh_dataset = defaultdict(lambda : None)\n",
    "sh_dataset['target_names'] =['no','yes']\n",
    "sh_dataset['target'] =[]\n",
    "sh_dataset['data'] =[]\n",
    "\n",
    "real_dataset = defaultdict(lambda : None)\n",
    "real_dataset['target_names'] =['no','yes']\n",
    "real_dataset['target'] =[]\n",
    "real_dataset['data'] =[]\n",
    "\n",
    "\n",
    "\n",
    "with open(datafile1, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(duplicate_samples_pos):\n",
    "                    sh_dataset['target'].append(1)\n",
    "                    sh_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                sh_dataset['target'].append(0)  \n",
    "                sh_dataset['data'].append(row['text'])\n",
    "with open(datafile2, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(duplicate_samples_pos):\n",
    "                    sh_dataset['target'].append(1)\n",
    "                    sh_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                sh_dataset['target'].append(0)  \n",
    "                sh_dataset['data'].append(row['text'])\n",
    "\n",
    "with open(datafile3, 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for i in range(duplicate_samples):\n",
    "            if float(row['class']) > 0:\n",
    "            #if row['class'] ==\"yes\":\n",
    "                for j in range(1):\n",
    "                    real_dataset['target'].append(1)\n",
    "                    real_dataset['data'].append(row['text'])\n",
    "            else:\n",
    "                real_dataset['target'].append(0)  \n",
    "                real_dataset['data'].append(row['text'])\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples:  1999999\n",
      "Training samples:  1999999\n",
      "Test samples:  1000000\n",
      "Total negative samples:  2976289\n",
      "Total positive samples:  23710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nX_train= np.array(X_train,dtype='float64')\\nX_test= np.array(X_test,dtype='float64')\\ny_train= np.array(y_train,dtype='float64')\\ny_test= np.array(y_test,dtype='float64')\\n\\n\\n# acceptance test data converting to numpy array \\nX_acceptance= np.array(sh_acceptance['data'],dtype='float64')\\ny_acceptance= np.array(sh_acceptance['target'],dtype='float64')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(sh_dataset['data'], sh_dataset['target'],\n",
    "    test_size=0.0)\n",
    "\n",
    "X_test,  y_test = real_dataset['data'], real_dataset['target']\n",
    "\n",
    "\n",
    "print \"Total samples: \",len(sh_dataset['data'])\n",
    "print \"Training samples: \",len(X_train)\n",
    "print \"Test samples: \",len(X_test)\n",
    "\n",
    "\n",
    "print \"Total negative samples: \",y_train.count(0)+y_test.count(0)\n",
    "print \"Total positive samples: \",y_train.count(1)+y_test.count(1)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "X_train= np.array(X_train,dtype='float64')\n",
    "X_test= np.array(X_test,dtype='float64')\n",
    "y_train= np.array(y_train,dtype='float64')\n",
    "y_test= np.array(y_test,dtype='float64')\n",
    "\n",
    "\n",
    "# acceptance test data converting to numpy array \n",
    "X_acceptance= np.array(sh_acceptance['data'],dtype='float64')\n",
    "y_acceptance= np.array(sh_acceptance['target'],dtype='float64')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999999, 100)\n",
      "(1000000, 100)\n"
     ]
    }
   ],
   "source": [
    "#Process vocabulary\n",
    "\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "\n",
    "char_processor = skflow.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "X_train = np.array(list(char_processor.fit_transform(X_train)))\n",
    "X_test = np.array(list(char_processor.transform(X_test)))\n",
    "\n",
    "print X_train.shape\n",
    "#print y_train.shape\n",
    "print X_test.shape\n",
    "#print y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Character models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HIDDEN_SIZE = 50\n",
    "HIDDEN_SIZE = 10\n",
    "\n",
    "N_FILTERS = 10\n",
    "FILTER_SHAPE1 = [20, 256]\n",
    "FILTER_SHAPE2 = [20, N_FILTERS]\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "\n",
    "\n",
    "def char_rnn_model(X, y):\n",
    "    byte_list = skflow.ops.one_hot_matrix(X, 256)\n",
    "    byte_list = skflow.ops.split_squeeze(1, MAX_DOCUMENT_LENGTH, byte_list)\n",
    "    cell = rnn_cell.GRUCell(HIDDEN_SIZE)\n",
    "    #cell = rnn_cell.BasicLSTMCell(HIDDEN_SIZE)\n",
    "    _, encoding = rnn.rnn(cell, byte_list, dtype=tf.float32)\n",
    "    return skflow.models.logistic_regression(encoding, y)\n",
    "    \n",
    "\n",
    "\n",
    "def char_cnn_model(X, y):\n",
    "    \"\"\"Character level convolutional neural network model to predict classes.\"\"\"\n",
    "    byte_list = tf.reshape(skflow.ops.one_hot_matrix(X, 256), \n",
    "        [-1, MAX_DOCUMENT_LENGTH, 256, 1])\n",
    "    with tf.variable_scope('CNN_Layer1'):\n",
    "        # Apply Convolution filtering on input sequence.\n",
    "        conv1 = skflow.ops.conv2d(byte_list, N_FILTERS, FILTER_SHAPE1, padding='VALID')\n",
    "        # Add a RELU for non linearity.\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        # Max pooling across output of Convlution+Relu.\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, POOLING_WINDOW, 1, 1], \n",
    "            strides=[1, POOLING_STRIDE, 1, 1], padding='SAME')\n",
    "        # Transpose matrix so that n_filters from convolution becomes width.\n",
    "        pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n",
    "    with tf.variable_scope('CNN_Layer2'):\n",
    "        # Second level of convolution filtering.\n",
    "        conv2 = skflow.ops.conv2d(pool1, N_FILTERS, FILTER_SHAPE2,\n",
    "            padding='VALID')\n",
    "        # Max across each filter to get useful features for classification.\n",
    "        pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])\n",
    "    # Apply regular WX + B and classification.\n",
    "    return skflow.models.logistic_regression(pool2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure GPU \n",
    "config_addon = skflow.addons.ConfigAddon(num_cores=5, gpu_memory_fraction=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'histogram_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2aed5258da30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# without early stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/skflow/estimators/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, monitor, logdir)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;31m# Sets up model and trainer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m             \u001b[1;31m# Initialize model parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/skflow/estimators/base.pyc\u001b[0m in \u001b[0;36m_setup_training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;31m# Add histograms for X and y if they are floats.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_feeder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_feeder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'histogram_summary'"
     ]
    }
   ],
   "source": [
    "#early stop set\n",
    "val_monitor = skflow.monitors.ValidationMonitor(X_train, y_train,\n",
    "                                                early_stopping_rounds=200,\n",
    "                                                n_classes=2,\n",
    "                                                print_steps=50)\n",
    "\n",
    "classifier = skflow.TensorFlowEstimator(model_fn=char_rnn_model, n_classes=2,\n",
    "    steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True, \n",
    "                                        config_addon=config_addon)\n",
    "\n",
    "\n",
    "count=0\n",
    "while count<10:\n",
    "    # with early stop\n",
    "    #classifier.fit(X_train, y_train, val_monitor, logdir='char_rnn')\n",
    "    \n",
    "    # without early stop\n",
    "    classifier.fit(X_train, y_train, logdir='char_rnn')\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, classifier.predict(X_test,batch_size=10))\n",
    "    print('Accuracy: {0:f}'.format(score))\n",
    "    count+=1\n",
    "    \n",
    "\n",
    "print \"\\n\\nMore details:\"\n",
    "predicted = classifier.predict(X_test,batch_size=10)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print \"Confusion Matrix\"\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "print(cm)\n",
    "\n",
    "print \"Done\"\n",
    "    \n",
    "'''\n",
    "# Predicting based on acceptance dataset\n",
    "print \"\\n\\nFor Acceptance test:\"\n",
    "predicted = classifier.predict(X_acceptance)\n",
    "print(metrics.classification_report(y_acceptance, predicted))\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print \"Confusion Matrix\"\n",
    "cm = metrics.confusion_matrix(y_acceptance, predicted)\n",
    "print(cm)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\n\\nMore details:\"\n",
    "predicted = classifier.predict(X_test[:15000])\n",
    "print(metrics.classification_report(y_test[:15000], predicted[:15000]))\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print \"Confusion Matrix\"\n",
    "cm = metrics.confusion_matrix(y_test[:15000], predicted[:15000])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Process input data for classifier based on 15 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bunch(dict):\n",
    "    \"\"\"Container object for datasets: dictionary-like object that\n",
    "        exposes its keys as attributes.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        dict.__init__(self, kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "def text_delexicalization(text):\n",
    "    \"\"\"delexicalization of each text string\n",
    "    \"\"\"\n",
    "    regular_num = \"#number \"\n",
    "    pattern_reg = re.compile('^(\\d+(\\.\\d+)*(\\.)?)|([a-z]+\\.\\s)', re.IGNORECASE)\n",
    "    rep_text= pattern_reg.sub(regular_num,text)\n",
    "    return rep_text\n",
    "\n",
    "def generate_dataset(ann_file):\n",
    "\n",
    "    target = []\n",
    "    target_names = ['no','yes']\n",
    "    feature_names = [\"pos_nnp\",\"without_verb_higher_line_space\",\"font_weight\",\"bold_italic\",\"at_least_3_lines_upper\",\"higher_line_space\",'number_dot','text_len_group','seq_number','references_appendix','header_0','header_1','header_2',\"title_case\",\"all_upper\"]\n",
    "    #feature_names = [\"font_weight\",\"bold_italic\",'number_dot','text_len_group','header_0','header_1','header_2',\"title_case\"]\n",
    "    rawtext = []\n",
    "    no_delex_rawtext =[]\n",
    "    data =[]\n",
    "    file_names=[]\n",
    "    auxiliary_verb = [\"is\",\"was\",\"were\",\"am\",\"are\",\"may\",\"might\",\"be\",\"will\",\"shall\",\"should\",\"must\",\"need\",\"have\",\"can\",\"could\",\"ought\",\"would\"]\n",
    "    one_letter=list(string.ascii_uppercase)+list(string.ascii_lowercase)\n",
    "    one_letter.append(\"I\")\n",
    "    one_letter.append(\"II\")\n",
    "    one_letter.append(\"III\")\n",
    "    one_letter.append(\"IV\")\n",
    "    one_letter.append(\"V\")\n",
    "    one_letter.append(\"VI\")\n",
    "    one_letter.append(\"VII\")\n",
    "    one_letter.append(\"VIII\")\n",
    "    one_letter.append(\"IX\")\n",
    "    one_letter.append(\"X\")\n",
    "\n",
    "\n",
    "    all_json_objs={}\n",
    "    with open(ann_file, 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            #all_json_objs[row['file_name']] = row\n",
    "            all_json_objs.setdefault(row['file_name'],[]).append(row)\n",
    "\n",
    "\n",
    "    for reader in all_json_objs:\n",
    "        each_file_json= all_json_objs[reader]\n",
    "        print \"processing file \"+ reader\n",
    "        #for row in each_file_json:\n",
    "        all_font_weights=[]\n",
    "        all_font_size=[]\n",
    "        avg_font_weight =0.0\n",
    "        avg_font_size =0.0\n",
    "        avg_line_space =0.0\n",
    "        minimum_line_space =100.0\n",
    "        line_index=0\n",
    "        counted_lines=0            \n",
    "\n",
    "        for line in each_file_json:\n",
    "            all_font_weights.append(float(line['font-weight']))\n",
    "            all_font_size.append(float(line['font_size']))\n",
    "            avg_font_weight += float(line['font-weight'])\n",
    "            avg_font_size += float(line['font_size'])\n",
    "            # line space \n",
    "            if line_index < len(each_file_json)-1:\n",
    "                if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]: \n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>50:\n",
    "                        continue\n",
    "                    avg_line_space += abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))<minimum_line_space:\n",
    "                        minimum_line_space = abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))\n",
    "                    counted_lines += 1\n",
    "                    \"\"\"\n",
    "                    print each_file_json[line_index][\"y-pos-l\"]\n",
    "                    print each_file_json[line_index+1][\"y-pos-l\"]\n",
    "                    print \"page \",each_file_json[line_index][\"page-number\"]\n",
    "                    print \"next page \" ,each_file_json[line_index+1][\"page-number\"]\n",
    "                    \"\"\"\n",
    "\n",
    "            line_index += 1\n",
    "        if counted_lines !=0:\n",
    "            avg_line_space = avg_line_space/counted_lines    \n",
    "        avg_font_size = avg_font_size/len(each_file_json)\n",
    "        avg_font_weight = avg_font_weight/len(each_file_json)\n",
    "        #print avg_line_space\n",
    "        #print minimum_line_space\n",
    "\n",
    "\n",
    "\n",
    "        font_weight_counter = defaultdict(int)\n",
    "        for word in all_font_weights:  \n",
    "            font_weight_counter[word] += 1\n",
    "        font_weight_counter = sorted(font_weight_counter, key = font_weight_counter.get, reverse = True)\n",
    "        #print font_weight_counter[0]\n",
    "\n",
    "        font_size_counter = defaultdict(int)\n",
    "        for word in all_font_size:  \n",
    "            font_size_counter[word] += 1            \n",
    "        font_size_counter = sorted(font_size_counter, key = font_size_counter.get, reverse = True)\n",
    "        #print font_size_counter[0]\n",
    "\n",
    "\n",
    "        #print json.dumps(json_obj, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "        #exit()\n",
    "        line_index=0\n",
    "        for line in each_file_json:\n",
    "            each_element={}\n",
    "            each_element[\"text\"]= line[\"text\"].strip()\n",
    "\n",
    "            # check line starts with a number or not\n",
    "            if line[\"text\"].decode('utf-8').split(\" \")[0].replace(\".\",\"\").isdigit() or line[\"text\"].decode('utf-8').split(\" \")[0] in one_letter:\n",
    "                if len(line[\"text\"].split())<5:\n",
    "                    each_element[\"text_len_group\"]=1\n",
    "                elif len(line[\"text\"].split())<7:\n",
    "                    each_element[\"text_len_group\"]=2\n",
    "                else:\n",
    "                    each_element[\"text_len_group\"]=3\n",
    "            else:\n",
    "                if len(line[\"text\"].split())<4:\n",
    "                    each_element[\"text_len_group\"]=1\n",
    "                elif len(line[\"text\"].split())<6:\n",
    "                    each_element[\"text_len_group\"]=2\n",
    "                else:\n",
    "                    each_element[\"text_len_group\"]=3   \n",
    "\n",
    "            #if \":\" in line[\"text\"].decode('utf-8'):\n",
    "            if re.match(\"^(references|appendix)\",line[\"text\"],re.IGNORECASE):\n",
    "                each_element[\"references_appendix\"]=1\n",
    "            else:    \n",
    "                each_element[\"references_appendix\"]=0\n",
    "\n",
    "            #if re.match(\"((\\d+|[a-z])\\s?\\.)\",line[\"text\"],re.IGNORECASE):\n",
    "            if re.match(\"^\\d+(\\s|\\.)+(\\d+(\\s|\\.)+)*[a-z]+\",line[\"text\"],re.IGNORECASE) or re.match(\"^[a-z](\\s\\.\\s)\",line[\"text\"],re.IGNORECASE):\n",
    "                each_element[\"number_dot\"]=1\n",
    "            else:\n",
    "                each_element[\"number_dot\"]=0                    \n",
    "\n",
    "            #if re.match(\"((\\d+|(IX|IV|V?I{0,3}))\\s?(\\.|\\))(\\d*))\",line[\"text\"],re.IGNORECASE):\n",
    "            #if re.match(\"(\\d+|(([MDCLXVI])M*(C[MD]|D?C*)(X[CL]|L?X*)(I[XV]|V?I*)))(\\s|\\.|\\))?\\d*\",line[\"text\"],re.IGNORECASE):\n",
    "            if re.match(\"^([a-z]|(IX|IV|V?I{0,3}))(\\.|\\s)\",line[\"text\"],re.IGNORECASE):\n",
    "                each_element[\"seq_number\"]=1\n",
    "            else:\n",
    "                each_element[\"seq_number\"]=0    \n",
    "\n",
    "            # case features\n",
    "            each_element[\"at_least_3_lines_upper\"] = 0\n",
    "            if line[\"text\"].isupper():\n",
    "                each_element[\"all_upper\"]=1\n",
    "                if line_index > 0 and line_index< len(each_file_json)-1:\n",
    "                    if each_file_json[line_index-1][\"text\"].isupper() and each_file_json[line_index+1][\"text\"].isupper():                     \n",
    "                        each_element[\"at_least_3_lines_upper\"]=1   \n",
    "            else:\n",
    "                each_element[\"all_upper\"]=0    \n",
    "\n",
    "            #line[\"text\"]=\"2 Preliminaries and Main Results\"\n",
    "            count_title=0\n",
    "            for word in line[\"text\"].strip().decode('utf-8').split(\" \"):\n",
    "                if word.istitle():\n",
    "                    count_title+=1\n",
    "\n",
    "            # checking the first word as number and then increase title word by one\n",
    "            if line[\"text\"].strip().decode('utf-8').split(\" \")[0].replace(\".\",\"\").isdigit():\n",
    "                count_title+=1\n",
    "\n",
    "            if count_title/float(len(line[\"text\"].strip().decode('utf-8').split(\" \")))>0.50:\n",
    "                each_element[\"title_case\"]=1\n",
    "            else:\n",
    "                each_element[\"title_case\"]=0\n",
    "\n",
    "\n",
    "            #import ipdb\n",
    "            #ipdb.set_trace()\n",
    "\n",
    "            #print line[\"text\"]                \n",
    "            #print \"Count: \",count_title\n",
    "            #print \"Len: \",len(line[\"text\"].split(\" \"))\n",
    "            #print each_element[\"title_case\"]\n",
    "\n",
    "            #if re.sub(\"(\\d+|(([MDCLXVI])M*(C[MD]|D?C*)(X[CL]|L?X*)(I[XV]|V?I*)))(\\s|\\.|\\))?\\d*\",\"\",line[\"text\"].decode('utf-8')).istitle():\n",
    "            #    each_element[\"title_case\"]=1\n",
    "            #else:\n",
    "            #    each_element[\"title_case\"]=0\n",
    "\n",
    "            verb_flag =0 # no auxiliary verb\n",
    "            for verb in auxiliary_verb:\n",
    "                if verb in line[\"text\"].decode('utf-8').split(\" \"):\n",
    "                    verb_flag=1\n",
    "                    break\n",
    "\n",
    "            each_element[\"without_verb_higher_line_space\"] = 0\n",
    "            if verb_flag == 0:      \n",
    "                if line_index < len(each_file_json)-1 and line_index > 0:              \n",
    "                    if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"] and each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space and abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>minimum_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1\n",
    "                elif line_index > 0:\n",
    "                    if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>avg_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1        \n",
    "                elif line_index < len(each_file_json)-1:\n",
    "                    if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                        if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space:\n",
    "                            each_element[\"without_verb_higher_line_space\"] =1        \n",
    "\n",
    "            # only line spaceing \n",
    "            each_element[\"higher_line_space\"] = 0\n",
    "            if line_index < len(each_file_json)-1 and line_index > 0:              \n",
    "                if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"] and each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space and abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>minimum_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1\n",
    "            elif line_index > 0:\n",
    "                if each_file_json[line_index-1][\"page-number\"] == each_file_json[line_index][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index-1][\"y-pos-l\"]) - float(each_file_json[line_index][\"y-pos-l\"]))>avg_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1        \n",
    "            elif line_index < len(each_file_json)-1:\n",
    "                if each_file_json[line_index][\"page-number\"] == each_file_json[line_index+1][\"page-number\"]:\n",
    "                    if abs(float(each_file_json[line_index][\"y-pos-l\"]) - float(each_file_json[line_index+1][\"y-pos-l\"]))>avg_line_space:\n",
    "                        each_element[\"higher_line_space\"] =1        \n",
    "\n",
    "\n",
    "\n",
    "            #if float(line[\"font_size\"])>12.0:\n",
    "            if float(line[\"font_size\"])>font_size_counter[0]:\n",
    "            #if float(line[\"font_size\"])>avg_font_size:    \n",
    "                each_element[\"header_0\"] =1\n",
    "            else:\n",
    "                each_element[\"header_0\"] =0\n",
    "\n",
    "            #if float(line[\"font_size\"])>=12.0 and float(line[\"font-weight\"])>=300.0:\n",
    "            if float(line[\"font_size\"])>=font_size_counter[0] and float(line[\"font-weight\"])>font_weight_counter[0]:\n",
    "            #if float(line[\"font_size\"])>=avg_font_size and float(line[\"font-weight\"])>avg_font_weight:\n",
    "                each_element[\"header_1\"] =1\n",
    "            else:\n",
    "                each_element[\"header_1\"] =0\n",
    "\n",
    "            #if float(line[\"font_size\"]) >=12.0 and \"bold\" in line[\"font-family\"].lower():\n",
    "            if float(line[\"font_size\"]) >= font_size_counter[0] and \"bold\" in line[\"font-family\"].lower():    \n",
    "            #if float(line[\"font_size\"]) >= avg_font_size and \"bold\" in line[\"font-family\"].lower():\n",
    "                each_element[\"header_2\"] =1\n",
    "            else:\n",
    "                each_element[\"header_2\"] =0\n",
    "\n",
    "            if float(line[\"font-weight\"])>font_weight_counter[0]:\n",
    "                each_element[\"font_weight\"] =1\n",
    "            else:\n",
    "                each_element[\"font_weight\"] =0\n",
    "\n",
    "            if \"bold\" in line[\"font-family\"].lower() and \"italic\" in line[\"font-family\"].lower():\n",
    "                each_element[\"bold_italic\"] =1\n",
    "            else:\n",
    "                each_element[\"bold_italic\"] =0\n",
    "\n",
    "\n",
    "            # POS tagging\n",
    "            tokens = nltk.word_tokenize(line[\"text\"].decode('utf-8'))\n",
    "            text = nltk.Text(tokens)\n",
    "            tags = nltk.pos_tag(text) \n",
    "            counts = Counter(tag for word,tag in tags)\n",
    "            total_pos = sum(counts.values())\n",
    "            pos = dict((word, float(count)/total_pos) for word,count in counts.items())\n",
    "\n",
    "            if \"NNP\" in pos.keys() and \"NN\" in pos.keys():\n",
    "                if pos[\"NNP\"] + pos[\"NN\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            elif \"NNP\" in pos.keys():\n",
    "                if pos[\"NNP\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            elif \"NN\" in pos.keys():\n",
    "                if pos[\"NN\"]  > 0.5:\n",
    "                    each_element[\"pos_nnp\"]=1\n",
    "                else:\n",
    "                    each_element[\"pos_nnp\"]=0\n",
    "            else:\n",
    "                each_element[\"pos_nnp\"]=0\n",
    "\n",
    "            if line['class'] ==\"0\":\n",
    "                target.append(0)\n",
    "            else:\n",
    "                target.append(1)    \n",
    "\n",
    "            data.append([each_element[\"pos_nnp\"],each_element[\"without_verb_higher_line_space\"],each_element[\"font_weight\"],each_element[\"bold_italic\"],each_element[\"at_least_3_lines_upper\"],each_element[\"higher_line_space\"],each_element['number_dot'],each_element['text_len_group'],each_element['seq_number'],each_element['references_appendix'],each_element['header_0'],each_element['header_1'],each_element['header_2'],each_element[\"title_case\"],each_element[\"all_upper\"]])\n",
    "\n",
    "            #data.append([each_element[\"font_weight\"],each_element[\"bold_italic\"],each_element['number_dot'],each_element['text_len_group'],each_element['header_0'],each_element['header_1'],each_element['header_2'],each_element[\"title_case\"]])\n",
    "            #rawtext.append(self.text_delexicalization(each_element['text']))\n",
    "            rawtext.append(each_element['text'])\n",
    "            #no_delex_rawtext.append(each_element['text'])\n",
    "            file_names.append(reader)\n",
    "            line_index += 1\n",
    "\n",
    "            #import ipdb\n",
    "            #ipdb.set_trace()\n",
    "\n",
    "            #print line[\"text\"],count_title,len(line[\"text\"].strip().decode('utf-8').split(\" \")), data[-1]\n",
    "    return Bunch(data=data, feature_names=feature_names,target_names=target_names,target=target,rawtext=rawtext,filenames=file_names)        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need the text field, though it generate 15 features. because 15 features ar enot required for deep learning.\n",
    "This is just for the test on real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dataset = generate_dataset(\"testset_acrobat_section_header.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for test data \n",
    "sh_test = defaultdict(lambda : None)\n",
    "sh_test['target_names'] =['no','yes']\n",
    "sh_test['feature_names'] = [\"pos_nnp\",\"without_verb_higher_line_space\",\"font_weight\",\"bold_italic\",\"at_least_3_lines_upper\",\"higher_line_space\",'number_dot','text_len_group','seq_number','references_appendix','header_0','header_1','header_2',\"title_case\",\"all_upper\"]\n",
    "sh_test['target'] =[]\n",
    "sh_test['data'] =[]\n",
    "\n",
    "\n",
    "for row in test_dataset.rawtext:\n",
    "    #sh_test['target'].append(int(row['class']))\n",
    "    sh_test['data'].append(row)\n",
    "    \n",
    "\n",
    "print \"Testing samples: \",len(sh_test['data'])    \n",
    "# test data converting to numpy array \n",
    "#X_test= np.array(sh_test['data'],dtype='float64')\n",
    "X_test = np.array(list(char_processor.transform(sh_test['data'])))\n",
    "\n",
    "print X_test.shape\n",
    "# Predicting based on acceptance dataset\n",
    "print \"\\n\\nTesting the classifier:\"\n",
    "predicted = classifier.predict(X_test)\n",
    "out_file = codecs.open(\"result_section_header_rnn_more_epochs.txt\", \"w\",encoding=\"utf-8\")\n",
    "unique_file_list=[]\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i] ==1:\n",
    "        if test_dataset.filenames[i].split(\".tetml\")[0] not in unique_file_list:\n",
    "            unique_file_list.append(test_dataset.filenames[i].split(\".tetml\")[0])\n",
    "            out_file.write(\"\\n\\n\")\n",
    "            out_file.write(test_dataset.filenames[i].split(\".tetml\")[0])\n",
    "            out_file.write(\"\\n\")\n",
    "            out_file.write(\"======================================================\")\n",
    "            out_file.write(\"\\n\")\n",
    "            out_file.write(test_dataset.no_delex_rawtext[i].decode('utf-8'))\n",
    "            out_file.write(\"\\n\")\n",
    "        else:\n",
    "            out_file.write(test_dataset.no_delex_rawtext[i].decode('utf-8'))\n",
    "            out_file.write(\"\\n\")\n",
    "\n",
    "out_file.close()                \n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
