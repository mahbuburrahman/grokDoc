{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a section sequencing classifier. We used LSTM for section sequencing.\n",
    "\n",
    "https://stackoverflow.com/questions/38189070/how-do-i-create-a-variable-length-input-lstm-in-keras\n",
    "\n",
    "https://machinelearningmastery.com/sequence-prediction/\n",
    "\n",
    "https://stackoverflow.com/questions/43117654/many-to-many-sequence-prediction-with-different-sequence-length\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/how-to-use-an-encoder-decoder-lstm-to-echo-sequences-of-random-integers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as skprep\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import keras\n",
    "from tensorflow.contrib.keras import models\n",
    "from tensorflow.contrib.keras import datasets\n",
    "from tensorflow.contrib.keras import layers\n",
    "from tensorflow.contrib.keras import preprocessing\n",
    "from tensorflow.contrib.keras import backend as K\n",
    "from tensorflow.contrib.keras import callbacks\n",
    "from tensorflow.contrib.keras import utils\n",
    "\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from itertools import repeat, chain, islice\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn import metrics,cross_validation\n",
    "from tensorflow.contrib.keras.python.keras.layers.wrappers import Bidirectional\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a sequence of random numbers in [0, 99]\n",
    "def generate_sequence(length=15):\n",
    "    return [randint(0, 19) for _ in range(length)]\n",
    "\n",
    "# one hot encode sequence\n",
    "def one_hot_encode(sequence, n_unique=20):\n",
    "    encoding = list()\n",
    "    for value in sequence:\n",
    "        vector = [0 for _ in range(n_unique)]\n",
    "        vector[value] = 1\n",
    "        encoding.append(vector)\n",
    "    return array(encoding)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "    return [argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "# convert encoded sequence to supervised learning\n",
    "def to_supervised(sequence, n_in, n_out):\n",
    "    # create lag copies of the sequence\n",
    "    df = DataFrame(sequence)\n",
    "    df = concat([df.shift(n_in-i-1) for i in range(n_in)], axis=1)\n",
    "    # drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "    # specify columns for input and output pairs\n",
    "    values = df.values\n",
    "    width = sequence.shape[1]\n",
    "    X = values.reshape(len(values), n_in, width)\n",
    "    y = values[:, 0:(n_out*width)].reshape(len(values), n_out, width)\n",
    "    return X, y\n",
    "\n",
    "# prepare data for the LSTM\n",
    "def get_data(n_in, n_out):\n",
    "    # generate random sequence\n",
    "    sequence = generate_sequence()\n",
    "    # one hot encode\n",
    "    encoded = one_hot_encode(sequence)\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    # convert to X,y pairs\n",
    "    X,y = to_supervised(encoded, n_in, n_out)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.80\n",
    "#config.gpu_options.allow_growth=True\n",
    "config.gpu_options.visible_device_list=\"0\"\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "K.gpu_setup = [\"gpu0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encdoing labels into numbers\n",
    "class_list={}\n",
    "for x in open(\"data/class_list.txt\",\"r\").readlines():\n",
    "    x=x.strip()\n",
    "    class_list[x.split(\":\")[0]]=x.split(\":\")[1].lower()\n",
    "\n",
    "lencoder = preprocessing.LabelEncoder()\n",
    "lencoder.fit_transform(class_list.values())\n",
    "\n",
    "model_dir=\"lstm_sec_sequencing_evaluation1/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "sys.stdout = open(model_dir+'report_sections_sequencing_classifier_lstm_evaluation_'+ \n",
    "                  str(datetime.now()).replace(\" \",\"-\").replace(\":\",\"-\")+'.txt', 'w')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process training and test data\n",
    "filler=19\n",
    "size=15\n",
    "X_train={}\n",
    "X_train[\"Sequence\"]=[]\n",
    "train_f=open(\"data/train_section_sequencing_classifier\",\"r\")\n",
    "\n",
    "for line in train_f:\n",
    "    line=list(lencoder.transform(line.strip().split(\",\")))\n",
    "    if len(line)<1:\n",
    "        continue\n",
    "        \n",
    "    X_train[\"Sequence\"].append(list(islice(chain(line, repeat(filler)), size)))\n",
    "\n",
    "#X_train = np.array(X_train[\"Sequence\"])\n",
    "X_train = X_train[\"Sequence\"]\n",
    "\n",
    "X_test={}\n",
    "X_test[\"Sequence\"]=[]\n",
    "test_f=open(\"data/test_section_sequencing_classifier\",\"r\")\n",
    "\n",
    "for line in test_f:\n",
    "    line=list(lencoder.transform(line.strip().split(\",\")))\n",
    "    if len(line)<1:\n",
    "        continue\n",
    "\n",
    "    X_test[\"Sequence\"].append(list(islice(chain(line, repeat(filler)), size)))\n",
    "\n",
    "#X_test= np.array(X_test[\"Sequence\"])\n",
    "X_test= X_test[\"Sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "memory_unit=20\n",
    "number_features=15  # dim of input sample. for example length of input vector.\n",
    "encoded_length = 20\n",
    "batch_size = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all numpy arraies of training and test\n",
    "n_in = 15\n",
    "n_out = 15\n",
    "\n",
    "# for training\n",
    "for index in range(len(X_train)):\n",
    "    sequence = X_train[index]\n",
    "    # one hot encode\n",
    "    encoded = one_hot_encode(sequence)\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    # convert to X,y pairs\n",
    "    X,y = to_supervised(encoded, n_in, n_out)\n",
    "    if index ==0:\n",
    "        XX_train=X\n",
    "        yy_train=y\n",
    "    else:\n",
    "        XX_train=np.vstack((XX_train,X))\n",
    "        yy_train=np.vstack((yy_train,X))\n",
    "    \n",
    "    \n",
    "# for test\n",
    "for index in range(len(X_test)):\n",
    "    sequence = X_test[index]\n",
    "    # one hot encode\n",
    "    encoded = one_hot_encode(sequence)\n",
    "    # convert to X,y pairs\n",
    "    X,y = to_supervised(encoded, n_in, n_out)\n",
    "    if index ==0:\n",
    "        XX_test=X\n",
    "        yy_test=y\n",
    "    else:\n",
    "        XX_test=np.vstack((XX_test,X))\n",
    "        yy_test=np.vstack((yy_test,X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(XX_train,open(model_dir+\"XX_train\",'wb'))\n",
    "pickle.dump(yy_train,open(model_dir+\"yy_train\",'wb'))\n",
    "pickle.dump(XX_test,open(model_dir+\"XX_test\",'wb'))\n",
    "pickle.dump(yy_test,open(model_dir+\"yy_test\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=model_dir+\"weights-improvement-{epoch:02d}-{val_loss:.9f}.hdf5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = callbacks.History()\n",
    "\n",
    "tb_callback = callbacks.TensorBoard(log_dir=model_dir, histogram_freq=50, write_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define LSTM\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(memory_unit, input_shape=(n_in, encoded_length), return_sequences=True, stateful=False))\n",
    "model.add(layers.TimeDistributed(layers.Dense(encoded_length, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print model.summary()\n",
    "\n",
    "# train LSTM\n",
    "# fit model for one epoch on this sequence\n",
    "model.fit(XX_train, yy_train, epochs=n_epoch, batch_size=batch_size, validation_data=(XX_test, yy_test),\n",
    "          verbose=1, shuffle=False,callbacks=[tb_callback, checkpoint,history])\n",
    "#model.reset_states()\n",
    "\n",
    "print \"Training Done\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate LSTM\n",
    "yhat = model.predict(XX_test, batch_size=batch_size, verbose=0)\n",
    "# decode all pairs\n",
    "correct = 0\n",
    "for i in range(len(XX_test)):\n",
    "    print('Expected:', one_hot_decode(yy_test[i]), 'Predicted', one_hot_decode(yhat[i]))\n",
    "    if one_hot_decode(yy_test[i]) == one_hot_decode(yhat[i]):\n",
    "        correct+=1\n",
    "\n",
    "\n",
    "print \"correct samples: \", correct\n",
    "print \"Total test samples: \", len(XX_test)\n",
    "print \"Test Accuracy: \", correct/len(XX_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(model_dir+\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #load training and test data\n",
    "# model_dir=\"lstm_sec_sequencing/\"\n",
    "# XX_train = pickle.load(open(model_dir+\"XX_train\",'rb'))\n",
    "# yy_train = pickle.load(open(model_dir+\"yy_train\",'rb'))\n",
    "# XX_test = pickle.load(open(model_dir+\"XX_test\",'rb'))\n",
    "# yy_test = pickle.load(open(model_dir+\"yy_test\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
