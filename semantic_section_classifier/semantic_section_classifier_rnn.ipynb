{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Bidirectional LSTM (RNN) based semantic section classiifer based on the classes from ontoogy.\n",
    "We implmented this classifier based on word based cnn and using keras backened tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import keras\n",
    "from tensorflow.contrib.keras import models\n",
    "from tensorflow.contrib.keras import datasets\n",
    "from tensorflow.contrib.keras import layers\n",
    "from tensorflow.contrib.keras import preprocessing\n",
    "from tensorflow.contrib.keras import backend as K\n",
    "from tensorflow.contrib.keras import callbacks\n",
    "from tensorflow.contrib.keras import utils\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn import metrics,cross_validation\n",
    "from tensorflow.contrib.keras.python.keras.layers.wrappers import Bidirectional\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90\n",
    "config.gpu_options.visible_device_list=\"0\"\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "K.gpu_setup = [\"gpu0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 0 # will be replace by the voc size after processing the input text\n",
    "maxlen = 200\n",
    "batch_size = 100\n",
    "embedding_dims = 100\n",
    "filters = 100\n",
    "kernel_size = 3\n",
    "hidden_dims = 100\n",
    "epochs = 200\n",
    "title_cat_aspects=\"data/Stratified_whole_semantic_section_classifier\"\n",
    "training_cat_aspects=\"data/Stratified_training_semantic_section_classifier\"\n",
    "validation_cat_aspects=\"data/Stratified_test_semantic_section_classifier\"\n",
    "max_count=999999999\n",
    "\n",
    "model_dir=\"LSTM_models_semantic_higher_input_length/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "embeddings_layer_names=None\n",
    "model_name=\"lstm_semantic_embedding_ep\"+str(epochs)+\"_em_dim\"+str(embedding_dims)\n",
    "chunksize = 10 ** 5\n",
    "\n",
    "sys.stdout = open(model_dir+'report_section_header_classifier'+ str(datetime.now()).replace(\" \",\"-\").replace(\":\",\"-\")\n",
    "                  +'.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_dataset = defaultdict(lambda : None)\n",
    "item_dataset['target'] =[]   # target is the category id \n",
    "item_dataset['title'] =[]\n",
    "\n",
    "label_cat_id_to_int={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_chunks_to_get_input_data(data_from_file,item_dataset):\n",
    "    for index, row in data_from_file.iterrows():\n",
    "#         if row[\"title\"] == \"\" or row[\"title\"] ==\"title\" or row.isnull().any():\n",
    "#             row = row.fillna(\"nan\")\n",
    "        try:\n",
    "            item_dataset['title'].append(row[\"title\"].lower())\n",
    "            # mapping category_id to int number for class labels\n",
    "            if row[\"category\"] in label_cat_id_to_int:\n",
    "                item_dataset['target'].append(label_cat_id_to_int[row[\"category\"]])\n",
    "            else:\n",
    "                label_cat_id_to_int[row[\"category\"]]=len(label_cat_id_to_int)\n",
    "                item_dataset['target'].append(label_cat_id_to_int[row[\"category\"]])\n",
    "                    \n",
    "        except:\n",
    "            \"\"\"\"\"\"\n",
    "\n",
    "def process_chunks_to_get_test_data(data_from_file,item_dataset_test):\n",
    "    for index, row in data_from_file.iterrows():\n",
    "#         if row[\"title\"] == \"\" or row[\"title\"] ==\"title\" or row.isnull().any():\n",
    "#             row = row.fillna(\"nan\")\n",
    "        try:\n",
    "            item_dataset_test['title'].append(row[\"title\"].lower())\n",
    "            item_dataset_test['target'].append(label_cat_id_to_int[row[\"category\"]])\n",
    "                    \n",
    "        except:\n",
    "            \"\"\"\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_chunk=0\n",
    "for chunk in pd.read_csv(title_cat_aspects, sep=\"\\t\", chunksize=chunksize):\n",
    "    print \"Processing chunk: \",count_chunk+1\n",
    "    process_chunks_to_get_input_data(chunk,item_dataset)\n",
    "    #if count_chunk >=2:\n",
    "    #    break\n",
    "    count_chunk+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_count= len(item_dataset['target'])\n",
    "print \"total targets: \",max_count\n",
    "print \"unique targets: \", len(set(item_dataset['target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing \n",
    "print \"Processing training data for voc generation\"\n",
    "x_train,y_train = item_dataset['title'],item_dataset['target']\n",
    "del item_dataset # after getting training an test data delete the item_dataset to release memory.\n",
    "\n",
    "with open(model_dir+'x_train_dump.json', 'w') as fp:\n",
    "    json.dump(x_train, fp)\n",
    "\n",
    "with open(model_dir+'label_cat_id_to_int.json', 'w') as fp:\n",
    "    json.dump(label_cat_id_to_int, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Processing voc\"\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(maxlen,min_frequency=150)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "print \"voc processing done!\"\n",
    "x_train.dump(model_dir+\"x_train_np_dump.dat\")\n",
    "del x_train\n",
    "\n",
    "pickle.dump(vocab_processor, open(model_dir+\"vocab_processor\",\n",
    "                                 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load vocab_processor\n",
    "vocab_processor = pickle.load(open(model_dir+\"vocab_processor\",'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab_processor.vocabulary_)\n",
    "max_features= n_words\n",
    "print('Total voc: %d' % n_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_cat_id_to_int = json.load(open(model_dir+\"label_cat_id_to_int.json\",\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "\n",
    "input_title =  layers.Input(shape=(maxlen,), name = 'input_title')\n",
    "\n",
    "embedding_title= layers.Embedding(max_features,embedding_dims,\n",
    "                    input_length=maxlen)(input_title)\n",
    "\n",
    "bid_lstm1= Bidirectional(layers.LSTM(embedding_dims, dropout=0.2, recurrent_dropout=0.2,return_sequences=True\n",
    "                                           ))(embedding_title)\n",
    "bid_lstm2 = Bidirectional(layers.LSTM(embedding_dims))(bid_lstm1)\n",
    "\n",
    "main_output = layers.Dense(len(label_cat_id_to_int), activation='softmax', name='main_output')(bid_lstm2)\n",
    "\n",
    "model = models.Model(inputs=[input_title], outputs=[main_output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_callback = callbacks.TensorBoard(log_dir=model_dir, histogram_freq=50, write_graph=True,\n",
    "                          #embeddings_freq=50,\n",
    "                          embeddings_layer_names=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=model_dir+\"weights-improvement-{epoch:02d}-{val_acc:.9f}.hdf5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max'\n",
    "                                      )\n",
    "history = callbacks.History()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data_from_file(path, aspectTransformationDict):\n",
    "    result = {}\n",
    "    while True:\n",
    "        for training_data in pd.read_csv(path, chunksize=256, sep='\\t'):\n",
    "            x = pd.DataFrame(list(vocab_processor.transform(map(lambda i:i.lower(),list(training_data['title'].fillna(\"nan\")))))).as_matrix()\n",
    "            y = [label_cat_id_to_int[i] for i in training_data['category']]\n",
    "            y = keras.utils.to_categorical(y,num_classes=len(label_cat_id_to_int))\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generate_data_from_file(training_cat_aspects,label_cat_id_to_int),\n",
    "                            validation_data=generate_data_from_file(validation_cat_aspects,label_cat_id_to_int),\n",
    "                            steps_per_epoch=2304,\n",
    "                            validation_steps=1,\n",
    "                            epochs=epochs,callbacks=[tb_callback,checkpoint,history],\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(model_dir+\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process test data for p,r,f1-score calculation\n",
    "item_dataset_test = defaultdict(lambda : None)\n",
    "item_dataset_test['target'] =[]   # target is the category id \n",
    "item_dataset_test['title'] =[]\n",
    "\n",
    "count_chunk=0\n",
    "for chunk in pd.read_csv(validation_cat_aspects, sep=\"\\t\", chunksize=chunksize):\n",
    "    print \"Processing chunk: \",count_chunk+1\n",
    "    process_chunks_to_get_test_data(chunk,item_dataset_test)\n",
    "    #if count_chunk >=2:\n",
    "    #    break\n",
    "    count_chunk+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(list(vocab_processor.transform(item_dataset_test['title'])))\n",
    "x_test.dump(model_dir+\"x_test_np_dump.dat\")\n",
    "y_test = keras.utils.to_categorical(item_dataset_test['target'],num_classes=len(label_cat_id_to_int))\n",
    "pickle.dump(y_test, open(model_dir+\"y_test\",'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.asarray(model.predict(x_test)).argmax(1)\n",
    "predict = np.round(np.asarray(model.predict(x_test))).argmax(1)\n",
    "targ = y_test.argmax(1)\n",
    "\n",
    "print(metrics.classification_report(targ, predict))\n",
    "print metrics.confusion_matrix(targ, predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
