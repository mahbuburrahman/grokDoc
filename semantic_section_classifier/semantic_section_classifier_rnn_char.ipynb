{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Bidirectional LSTM (RNN) based semantic section classiifer based on the classes from ontoogy.\n",
    "We implmented this classifier based on word based cnn and using keras backened tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubur/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import keras\n",
    "from tensorflow.contrib.keras import models\n",
    "from tensorflow.contrib.keras import datasets\n",
    "from tensorflow.contrib.keras import layers\n",
    "from tensorflow.contrib.keras import preprocessing\n",
    "from tensorflow.contrib.keras import backend as K\n",
    "from tensorflow.contrib.keras import callbacks\n",
    "from tensorflow.contrib.keras import utils\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn import metrics,cross_validation\n",
    "from tensorflow.contrib.keras.python.keras.layers.wrappers import Bidirectional\n",
    "learn = tf.contrib.learn\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.85\n",
    "config.gpu_options.visible_device_list=\"0\"\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "K.gpu_setup = [\"gpu0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 0 # will be replace by the voc size after processing the input text\n",
    "maxlen = 600\n",
    "batch_size = 100\n",
    "embedding_dims = 100\n",
    "filters = 100\n",
    "kernel_size = 3\n",
    "hidden_dims = 100\n",
    "epochs = 100\n",
    "title_cat_aspects=\"data/Stratified_whole_semantic_section_classifier\"\n",
    "training_cat_aspects=\"data/Stratified_training_semantic_section_classifier\"\n",
    "validation_cat_aspects=\"data/Stratified_test_semantic_section_classifier\"\n",
    "max_count=999999999\n",
    "\n",
    "model_dir=\"LSTM_models_semantic_higher_input_length_char/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "embeddings_layer_names=None\n",
    "model_name=\"lstm_semantic_embedding_ep\"+str(epochs)+\"_em_dim\"+str(embedding_dims)\n",
    "chunksize = 10 ** 5\n",
    "\n",
    "sys.stdout = open(model_dir+'report_semantic_section_classifier_char'+ str(datetime.now()).replace(\" \",\"-\").replace(\":\",\"-\")\n",
    "                  +'.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_dataset = defaultdict(lambda : None)\n",
    "item_dataset['target'] =[]   # target is the category id \n",
    "item_dataset['title'] =[]\n",
    "\n",
    "label_cat_id_to_int={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_chunks_to_get_input_data(data_from_file,item_dataset):\n",
    "    for index, row in data_from_file.iterrows():\n",
    "#         if row[\"title\"] == \"\" or row[\"title\"] ==\"title\" or row.isnull().any():\n",
    "#             row = row.fillna(\"nan\")\n",
    "        try:\n",
    "            item_dataset['title'].append(row[\"title\"].lower())\n",
    "            # mapping category_id to int number for class labels\n",
    "            if row[\"category\"] in label_cat_id_to_int:\n",
    "                item_dataset['target'].append(label_cat_id_to_int[row[\"category\"]])\n",
    "            else:\n",
    "                label_cat_id_to_int[row[\"category\"]]=len(label_cat_id_to_int)\n",
    "                item_dataset['target'].append(label_cat_id_to_int[row[\"category\"]])\n",
    "                    \n",
    "        except:\n",
    "            \"\"\"\"\"\"\n",
    "\n",
    "def process_chunks_to_get_test_data(data_from_file,item_dataset_test):\n",
    "    for index, row in data_from_file.iterrows():\n",
    "        try:\n",
    "            item_dataset_test['title'].append(row[\"title\"].lower())\n",
    "            item_dataset_test['target'].append(label_cat_id_to_int[row[\"category\"]])\n",
    "                    \n",
    "        except:\n",
    "            \"\"\"\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_chunk=0\n",
    "for chunk in pd.read_csv(title_cat_aspects, sep=\"\\t\", chunksize=chunksize):\n",
    "    print \"Processing chunk: \",count_chunk+1\n",
    "    process_chunks_to_get_input_data(chunk,item_dataset)\n",
    "    #if count_chunk >=2:\n",
    "    #    break\n",
    "    count_chunk+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_count= len(item_dataset['target'])\n",
    "print \"total targets: \",max_count\n",
    "print \"unique targets: \", len(set(item_dataset['target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing \n",
    "print \"Processing training data for voc generation\"\n",
    "x_train,y_train = item_dataset['title'],item_dataset['target']\n",
    "del item_dataset # after getting training an test data delete the item_dataset to release memory.\n",
    "\n",
    "with open(model_dir+'x_train_dump.json', 'w') as fp:\n",
    "    json.dump(x_train, fp)\n",
    "\n",
    "with open(model_dir+'label_cat_id_to_int.json', 'w') as fp:\n",
    "    json.dump(label_cat_id_to_int, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Processing voc\"\n",
    "char_processor = learn.preprocessing.ByteProcessor(maxlen)\n",
    "x_train = np.array(list(char_processor.fit_transform(x_train)))\n",
    "print \"voc processing done!\"\n",
    "x_train.dump(model_dir+\"x_train_np_dump.dat\")\n",
    "del x_train\n",
    "\n",
    "pickle.dump(char_processor, open(model_dir+\"char_processor\",\n",
    "                                 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load vocab_processor\n",
    "#char_processor = pickle.load(open(model_dir+\"char_processor\",'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_char = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#label_cat_id_to_int = json.load(open(model_dir+\"label_cat_id_to_int.json\",\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "\n",
    "input_title =  layers.Input(shape=(maxlen,), name = 'input_title')\n",
    "\n",
    "embedding_title= layers.Embedding(max_char,embedding_dims,\n",
    "                    input_length=maxlen)(input_title)\n",
    "\n",
    "bid_lstm1= Bidirectional(layers.LSTM(embedding_dims, dropout=0.2, recurrent_dropout=0.2,return_sequences=True\n",
    "                                           ))(embedding_title)\n",
    "bid_lstm2 = Bidirectional(layers.LSTM(embedding_dims))(bid_lstm1)\n",
    "\n",
    "main_output = layers.Dense(len(label_cat_id_to_int), activation='softmax', name='main_output')(bid_lstm2)\n",
    "\n",
    "model = models.Model(inputs=[input_title], outputs=[main_output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_callback = callbacks.TensorBoard(log_dir=model_dir, histogram_freq=50, write_graph=True,\n",
    "                          #embeddings_freq=50,\n",
    "                          embeddings_layer_names=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=model_dir+\"weights-improvement-{epoch:02d}-{val_acc:.9f}.hdf5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max'\n",
    "                                      )\n",
    "history = callbacks.History()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data_from_file(path, aspectTransformationDict):\n",
    "    result = {}\n",
    "    while True:\n",
    "        for training_data in pd.read_csv(path, chunksize=128, sep='\\t'):\n",
    "            x = pd.DataFrame(list(char_processor.transform(map(lambda i:i.lower(),list(training_data['title'].fillna(\"nan\")))))).as_matrix()\n",
    "            y = [label_cat_id_to_int[i] for i in training_data['category']]\n",
    "            y = keras.utils.to_categorical(y,num_classes=len(label_cat_id_to_int))\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.keras.python.keras.callbacks.History at 0x7f3a3ef7e850>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generate_data_from_file(training_cat_aspects,label_cat_id_to_int),\n",
    "                            validation_data=generate_data_from_file(validation_cat_aspects,label_cat_id_to_int),\n",
    "                            steps_per_epoch=4608,\n",
    "                            validation_steps=1,\n",
    "                            epochs=epochs,callbacks=[tb_callback,checkpoint,history],\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(model_dir+\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process test data for p,r,f1-score calculation\n",
    "item_dataset_test = defaultdict(lambda : None)\n",
    "item_dataset_test['target'] =[]   # target is the category id \n",
    "item_dataset_test['title'] =[]\n",
    "\n",
    "count_chunk=0\n",
    "for chunk in pd.read_csv(validation_cat_aspects, sep=\"\\t\", chunksize=chunksize):\n",
    "    print \"Processing chunk: \",count_chunk+1\n",
    "    process_chunks_to_get_test_data(chunk,item_dataset_test)\n",
    "    #if count_chunk >=2:\n",
    "    #    break\n",
    "    count_chunk+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(list(char_processor.transform(item_dataset_test['title'])))\n",
    "x_test.dump(model_dir+\"x_test_np_dump.dat\")\n",
    "y_test = keras.utils.to_categorical(item_dataset_test['target'],num_classes=len(label_cat_id_to_int))\n",
    "pickle.dump(y_test, open(model_dir+\"y_test\",'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = np.asarray(model.predict(x_test)).argmax(1)\n",
    "predict = np.round(np.asarray(model.predict(x_test))).argmax(1)\n",
    "targ = y_test.argmax(1)\n",
    "\n",
    "print(metrics.classification_report(targ, predict))\n",
    "print metrics.confusion_matrix(targ, predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
